{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd568f28-179b-44cc-abb1-f793601955d1",
   "metadata": {},
   "source": [
    "# Simplest RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789e961-59fe-4727-84b8-f5514a91b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AuthenticationError\n",
    "from docx import Document\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546b083-bf5f-464d-82ad-e0f29acb585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# get api key from environment\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# create OpenAI client\n",
    "def create_client(api_key):\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        client.models.list()\n",
    "        return client\n",
    "    except AuthenticationError:\n",
    "        print(\"Incorrect API\")\n",
    "    return None\n",
    "\n",
    "client = create_client(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ecc5b-cf92-40a4-a007-b58fc1a5c595",
   "metadata": {},
   "source": [
    "### Chunking and embedding of input document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfe490-f047-4282-af5a-550bf130ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file path\n",
    "filePath=r\"../../data/example.docx\"\n",
    "\n",
    "# read file\n",
    "doc = Document(filePath)\n",
    "\n",
    "# declare lists\n",
    "chunks = []\n",
    "embeddings = []\n",
    "\n",
    "# text division\n",
    "for i in range(0, len(doc.paragraphs)):\n",
    "    chunk = doc.paragraphs[i].text\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# create embeddings\n",
    "for i in range(0, len(chunks)):\n",
    "    embedding = client.embeddings.create(\n",
    "        input = chunks[i],\n",
    "        model = \"text-embedding-3-small\"\n",
    "    )\n",
    "    embeddings.append(embedding.data[0].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c7b89-1704-44a9-b73d-f37ba5eaddde",
   "metadata": {},
   "source": [
    "### Query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7489d9-be73-4990-ba36-718b0859def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user query\n",
    "user_query = \"How many samples I need for to assume the normal distribution of the mean?\"\n",
    "\n",
    "# generate embedding\n",
    "response = client.embeddings.create(\n",
    "    input = user_query,\n",
    "    model = \"text-embedding-3-small\"\n",
    ")\n",
    "query_embedding = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd6ecd-7c4c-469d-a60d-1fb6288acacf",
   "metadata": {},
   "source": [
    "\"By default, the length of the embedding vector is 1536 for text-embedding-3-small or 3072 for text-embedding-3-large. To reduce the embedding's dimensions without losing its concept-representing properties, pass in the dimensions parameter.\" (https://platform.openai.com/docs/guides/embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb264bbc-3dfd-44d8-9d7d-2aa0ef07f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f35be-a96b-42e0-b7e2-14e5f3ef5703",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe0838-604d-4db6-8e40-6ac12971f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most similar id\n",
    "best_match_id = cosine_similarity(\n",
    "    np.array(embeddings), np.array(query_embedding).reshape(1,-1)\n",
    "    ).argmax()\n",
    "\n",
    "# print most similar text\n",
    "chunks[best_match_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e858622-00e0-48d1-971b-f2bb74474387",
   "metadata": {},
   "source": [
    "### Answer (Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f536b-7b66-4b95-9c35-b4a357c36b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "        give a comprehensive answer to the question.\n",
    "        Respond only to the question asked, response should be concise and relevant to the question.\n",
    "        Provide the number of the source document when relevant.\n",
    "        If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Context:\n",
    "    {chunks[best_match_id]}\n",
    "    ---\n",
    "    Now here is the question you need to answer.\n",
    "    \n",
    "    Question: {user_query}\"\"\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# get and print response\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
