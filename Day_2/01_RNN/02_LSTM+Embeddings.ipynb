{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Sentiment classification with word embeddings\n",
    "\n",
    "Words are different from images or even molecules, in that the meaning of a word is not represented by the letters that make up the word (the same way that the meaning of an image is represented by the pixels that make up the pixel).  \n",
    "Instead, <b>the meaning of words comes from how they are used in conjunction with other words.</b>  \n",
    "\n",
    "### GloVe, Global Vectors for Word Representation\n",
    "\n",
    "There are multiple versions of pre-trained GloVe word embeddings.  \n",
    "They differ in the <i>corpus</i> used to train the embedding, and the <i>size</i> of the embeddings.\n",
    "\n",
    "GloVe is a project Stanford NLP: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS USED TO CREATE A SUBSET OF THE WHOLE aclImdb DATASET\n",
    "# If the data/aclImdb_subset directory exists this cell does nothing.\n",
    "# SET THE VARIABLES IN THE MIDDLE OF THE CELL to create each subfolder.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "if not os.path.isdir(\"../../Data/aclImdb_subset/\"):\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/train/pos\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/train/neg\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/test/pos\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/test/neg\")))\n",
    "\n",
    "    train_pos_files = os.listdir(\"../../Data/aclImdb/train/pos\")\n",
    "    train_neg_files = os.listdir(\"../../Data/aclImdb/train/neg\")\n",
    "    test_pos_files = os.listdir(\"../../Data/aclImdb/test/pos\")\n",
    "    test_neg_files = os.listdir(\"../../Data/aclImdb/test/neg\")\n",
    "\n",
    "\n",
    "    # SET THESE 3 VARIABLES\n",
    "    train_or_test = 'test'\n",
    "    pos_or_neg = 'neg'\n",
    "    file_names = test_neg_files\n",
    "    ############################\n",
    "\n",
    "    in_folder = \"../../Data/aclImdb/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "    out_folder = \"../../Data/aclImdb_subset/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "\n",
    "    used_indexes = []\n",
    "    for i in range(int(len(os.listdir(in_folder))/10)):\n",
    "        index = random.randint(0, len(os.listdir(in_folder)))\n",
    "        while index in used_indexes:\n",
    "            index = random.randint(0, len(os.listdir(in_folder)))\n",
    "        file_ = in_folder + file_names[index]\n",
    "        shutil.copy(file_ , out_folder)\n",
    "        used_indexes.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from tensorflow import keras\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk_stopw = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Text Corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = \"../../Data/aclImdb_subset/\"\n",
    "labelToName = { 0 : 'neg', 1: 'pos' }\n",
    "def getMovies(split):\n",
    "    '''\n",
    "    outputs:\n",
    "    X_raw: lista di  recensioni\n",
    "    Y: array di target; len(Y)=len(X_raw)\n",
    "    '''\n",
    "    X_raw, Y  = [], []\n",
    "\n",
    "    for classIndex, directory in enumerate(['neg', 'pos']):\n",
    "        dirName = data + split + \"/\" + directory\n",
    "        for reviewFile in os.listdir(dirName):\n",
    "            with open (dirName + '/' + reviewFile, 'r', encoding='utf8') as f:\n",
    "                raw = f.read()\n",
    "                if (len(raw) == 0):\n",
    "                    continue\n",
    "            X_raw.append(raw)\n",
    "            Y.append(classIndex)\n",
    "    return X_raw, np.array(Y)\n",
    "\n",
    "# We will split later in train and val\n",
    "X_raw, Y = getMovies(split='train')\n",
    "\n",
    "X_raw_test, Y_test = getMovies(split='test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  2494  reviews;  min length =  81 , max length =  8969 , median 975.0 chars\n",
      "TEST:  2496  reviews;  min length =  32 , mac length =  12988 , median 961.0 chars\n",
      "\n",
      " \n",
      " TEXT \n",
      " A young scientist is trying to carry on his dead father's work on limb regeneration.His overbearing mother has convinced him that he murdered his own father and is monitoring his progress for her own evil purposes.A young doctor uses reptilian DNA he extracts from a large creature and when his arm is conveniently ripped off a few minutes later,he injects himself with his formula and grows a new murderous arm...Admittedly the special effects in \"Severed Ties\" are pretty good and grotesque,but the rest of the film is awful.The severed arm is behaving like a snake and kills few people.Big deal.The acting is mediocre and the climax is silly.3 out of 10. \n",
      " LABEL = neg\n"
     ]
    }
   ],
   "source": [
    "n_char_train = [len(x) for x in X_raw]\n",
    "n_char_test = [len(x) for x in X_raw_test]\n",
    "print('TRAIN: ', len(X_raw),' reviews; ','min length = ', min(n_char_train), ', max length = ',max(n_char_train), ', median',np.median(n_char_train), 'chars')\n",
    "print('TEST: ', len(X_raw_test),' reviews; ','min length = ', min(n_char_test), ', mac length = ',max(n_char_test), ', median',np.median(n_char_test), 'chars')\n",
    "\n",
    "print('\\n \\n TEXT \\n',X_raw[0],'\\n LABEL =', labelToName[Y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "lowcase, tokenize, remove punctuations, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(pos):\n",
    "    '''\n",
    "    Convert nltk.pos_tag() tags  so that they can be understood by pos tags by nltk.WordNetLemmatizer()\n",
    "    '''\n",
    "    if pos.startswith('J'):\n",
    "        return 'a' # o wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return 'v' # o wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return 'n' # o wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return 'r' # o wordnet.ADV\n",
    "    else:          \n",
    "        return 'n' # default  \n",
    "\n",
    "def txt_preprocessing(X, printa=False):\n",
    "    i = 0 #text to print\n",
    "    #lowcase\n",
    "    X = [x.lower() for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # tokenize\n",
    "    X = [RegexpTokenizer(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b').tokenize(x) for x in X] #or [re.findall(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b',x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # remove stop words\n",
    "    X = [(lambda x: [x_i for x_i in x if x_i not in nltk_stopw])(x) for x in X] # or list(map(lambda x: ([x_i for x_i in x if x_i not in nltk_stopw]),X))\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatization using POS\n",
    "    X = [nltk.pos_tag(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # POS tags to match nltk.WordNetLemmatizer()\n",
    "    X = [ (lambda x: [(x_i[0],get_pos(x_i[1])) for x_i in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatizzo\n",
    "    X = [(lambda x: [nltk.WordNetLemmatizer().lemmatize(w,p) for w,p in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # reshape as a list of sentences: [['this','is','string','1'], ['this','is','string','2']...] --> ['this is string 1','this is string 2'...]\n",
    "    X = [\" \".join(x) for x in X]\n",
    "    if printa: print(X[i])\n",
    "\n",
    "    return X\n",
    "\n",
    "#a=['thIs Film was#@ the ?worst Ever', 'I sAw,  !very good Films recently!']    \n",
    "#txt_preprocessing(a, printa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a young scientist is trying to carry on his dead father's work on limb regeneration.his overbearing mother has convinced him that he murdered his own father and is monitoring his progress for her own evil purposes.a young doctor uses reptilian dna he extracts from a large creature and when his arm is conveniently ripped off a few minutes later,he injects himself with his formula and grows a new murderous arm...admittedly the special effects in \"severed ties\" are pretty good and grotesque,but the rest of the film is awful.the severed arm is behaving like a snake and kills few people.big deal.the acting is mediocre and the climax is silly.3 out of 10. \n",
      "\n",
      "['young', 'scientist', 'trying', 'carry', 'his', 'dead', 'father', 'work', 'limb', 'regeneration', 'his', 'overbearing', 'mother', 'has', 'convinced', 'him', 'that', 'murdered', 'his', 'own', 'father', 'and', 'monitoring', 'his', 'progress', 'for', 'her', 'own', 'evil', 'purposes', 'young', 'doctor', 'uses', 'reptilian', 'dna', 'extracts', 'from', 'large', 'creature', 'and', 'when', 'his', 'arm', 'conveniently', 'ripped', 'off', 'few', 'minutes', 'later', 'injects', 'himself', 'with', 'his', 'formula', 'and', 'grows', 'new', 'murderous', 'arm', 'admittedly', 'the', 'special', 'effects', 'severed', 'ties', 'are', 'pretty', 'good', 'and', 'grotesque', 'but', 'the', 'rest', 'the', 'film', 'awful', 'the', 'severed', 'arm', 'behaving', 'like', 'snake', 'and', 'kills', 'few', 'people', 'big', 'deal', 'the', 'acting', 'mediocre', 'and', 'the', 'climax', 'silly', 'out'] \n",
      "\n",
      "['young', 'scientist', 'trying', 'carry', 'dead', 'father', 'work', 'limb', 'regeneration', 'overbearing', 'mother', 'convinced', 'murdered', 'father', 'monitoring', 'progress', 'evil', 'purposes', 'young', 'doctor', 'uses', 'reptilian', 'dna', 'extracts', 'large', 'creature', 'arm', 'conveniently', 'ripped', 'minutes', 'later', 'injects', 'formula', 'grows', 'new', 'murderous', 'arm', 'admittedly', 'special', 'effects', 'severed', 'ties', 'pretty', 'good', 'grotesque', 'rest', 'film', 'awful', 'severed', 'arm', 'behaving', 'like', 'snake', 'kills', 'people', 'big', 'deal', 'acting', 'mediocre', 'climax', 'silly'] \n",
      "\n",
      "[('young', 'JJ'), ('scientist', 'NN'), ('trying', 'VBG'), ('carry', 'NN'), ('dead', 'JJ'), ('father', 'NN'), ('work', 'NN'), ('limb', 'NN'), ('regeneration', 'NN'), ('overbearing', 'VBG'), ('mother', 'NN'), ('convinced', 'VBN'), ('murdered', 'VBD'), ('father', 'RBR'), ('monitoring', 'NN'), ('progress', 'NN'), ('evil', 'JJ'), ('purposes', 'NNS'), ('young', 'JJ'), ('doctor', 'NN'), ('uses', 'VBZ'), ('reptilian', 'JJ'), ('dna', 'NN'), ('extracts', 'NNS'), ('large', 'JJ'), ('creature', 'NN'), ('arm', 'NN'), ('conveniently', 'RB'), ('ripped', 'VBN'), ('minutes', 'NNS'), ('later', 'RBR'), ('injects', 'NNS'), ('formula', 'VBP'), ('grows', 'VBZ'), ('new', 'JJ'), ('murderous', 'JJ'), ('arm', 'NN'), ('admittedly', 'RB'), ('special', 'JJ'), ('effects', 'NNS'), ('severed', 'VBD'), ('ties', 'NNS'), ('pretty', 'RB'), ('good', 'JJ'), ('grotesque', 'NN'), ('rest', 'NN'), ('film', 'NN'), ('awful', 'JJ'), ('severed', 'VBD'), ('arm', 'JJ'), ('behaving', 'NN'), ('like', 'IN'), ('snake', 'NN'), ('kills', 'NNS'), ('people', 'NNS'), ('big', 'JJ'), ('deal', 'NN'), ('acting', 'VBG'), ('mediocre', 'NN'), ('climax', 'NN'), ('silly', 'RB')] \n",
      "\n",
      "[('young', 'a'), ('scientist', 'n'), ('trying', 'v'), ('carry', 'n'), ('dead', 'a'), ('father', 'n'), ('work', 'n'), ('limb', 'n'), ('regeneration', 'n'), ('overbearing', 'v'), ('mother', 'n'), ('convinced', 'v'), ('murdered', 'v'), ('father', 'r'), ('monitoring', 'n'), ('progress', 'n'), ('evil', 'a'), ('purposes', 'n'), ('young', 'a'), ('doctor', 'n'), ('uses', 'v'), ('reptilian', 'a'), ('dna', 'n'), ('extracts', 'n'), ('large', 'a'), ('creature', 'n'), ('arm', 'n'), ('conveniently', 'r'), ('ripped', 'v'), ('minutes', 'n'), ('later', 'r'), ('injects', 'n'), ('formula', 'v'), ('grows', 'v'), ('new', 'a'), ('murderous', 'a'), ('arm', 'n'), ('admittedly', 'r'), ('special', 'a'), ('effects', 'n'), ('severed', 'v'), ('ties', 'n'), ('pretty', 'r'), ('good', 'a'), ('grotesque', 'n'), ('rest', 'n'), ('film', 'n'), ('awful', 'a'), ('severed', 'v'), ('arm', 'a'), ('behaving', 'n'), ('like', 'n'), ('snake', 'n'), ('kills', 'n'), ('people', 'n'), ('big', 'a'), ('deal', 'n'), ('acting', 'v'), ('mediocre', 'n'), ('climax', 'n'), ('silly', 'r')] \n",
      "\n",
      "['young', 'scientist', 'try', 'carry', 'dead', 'father', 'work', 'limb', 'regeneration', 'overbear', 'mother', 'convince', 'murder', 'father', 'monitoring', 'progress', 'evil', 'purpose', 'young', 'doctor', 'use', 'reptilian', 'dna', 'extract', 'large', 'creature', 'arm', 'conveniently', 'rip', 'minute', 'later', 'injects', 'formula', 'grow', 'new', 'murderous', 'arm', 'admittedly', 'special', 'effect', 'sever', 'tie', 'pretty', 'good', 'grotesque', 'rest', 'film', 'awful', 'sever', 'arm', 'behaving', 'like', 'snake', 'kill', 'people', 'big', 'deal', 'act', 'mediocre', 'climax', 'silly'] \n",
      "\n",
      "young scientist try carry dead father work limb regeneration overbear mother convince murder father monitoring progress evil purpose young doctor use reptilian dna extract large creature arm conveniently rip minute later injects formula grow new murderous arm admittedly special effect sever tie pretty good grotesque rest film awful sever arm behaving like snake kill people big deal act mediocre climax silly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['young scientist try carry dead father work limb regeneration overbear mother convince murder father monitoring progress evil purpose young doctor use reptilian dna extract large creature arm conveniently rip minute later injects formula grow new murderous arm admittedly special effect sever tie pretty good grotesque rest film awful sever arm behaving like snake kill people big deal act mediocre climax silly']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all passages in txt_processing\n",
    "txt_preprocessing([X_raw[0]], printa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 minute run\n",
    "X = txt_preprocessing(X_raw)\n",
    "X_test = txt_preprocessing(X_raw_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A young scientist is trying to carry on his dead father's work on limb regeneration.His overbearing mother has convinced him that he murdered his own father and is monitoring his progress for her own evil purposes.A young doctor uses reptilian DNA he extracts from a large creature and when his arm is conveniently ripped off a few minutes later,he injects himself with his formula and grows a new murderous arm...Admittedly the special effects in \"Severed Ties\" are pretty good and grotesque,but the rest of the film is awful.The severed arm is behaving like a snake and kills few people.Big deal.The acting is mediocre and the climax is silly.3 out of 10. \n",
      "\n",
      " young scientist try carry dead father work limb regeneration overbear mother convince murder father monitoring progress evil purpose young doctor use reptilian dna extract large creature arm conveniently rip minute later injects formula grow new murderous arm admittedly special effect sever tie pretty good grotesque rest film awful sever arm behaving like snake kill people big deal act mediocre climax silly\n"
     ]
    }
   ],
   "source": [
    "print(X_raw[0],'\\n\\n',X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4907268170426065 0.5390781563126252\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test/Train Split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2, random_state=123)\n",
    "print(Y_train.mean(), Y_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with Glove. \n",
    "If not present, browse to https://nlp.stanford.edu/projects/glove/ and download glove.6B.zip.\n",
    "Unzip files and put in a new directory \"glove\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r',encoding=\"utf8\") as f:\n",
    "\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "    return words, word_to_vec_map\n",
    "\n",
    "\n",
    "# Load the GloVe word embeddings file\n",
    "glove_file = \"../../data/glove/glove.6B.100d.txt\"\n",
    "glove_words, glove_word2vec_map = read_glove_vecs(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vectorize_sentence(sentence):\n",
    "    lemmas = sentence.split()\n",
    "    vectors = []\n",
    "    for word in lemmas:\n",
    "        if word in glove_words:\n",
    "            vectors.append(glove_word2vec_map[word])\n",
    "        else:\n",
    "            vectors.append(glove_word2vec_map[\"unk\"])\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = [glove_vectorize_sentence(sentence) for sentence in X_train]\n",
    "X_val_glove = [glove_vectorize_sentence(sentence) for sentence in X_val]\n",
    "X_test_glove = [glove_vectorize_sentence(sentence) for sentence in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length is:  131\n"
     ]
    }
   ],
   "source": [
    "# pad to take all sequences to same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "lengths = [len(vecs) for vecs in X_train_glove]\n",
    "max_length = int(np.percentile(lengths, 70))  # 80th percentile\n",
    "print(\"max_length is: \", max_length)\n",
    "    \n",
    "X_train_glove_pad = pad_sequences(X_train_glove,\n",
    "                                  maxlen=max_length, dtype='float32',\n",
    "                                  padding='post', truncating='pre')\n",
    "X_val_glove_pad = pad_sequences(X_val_glove,\n",
    "                                maxlen=max_length,\n",
    "                                dtype='float32', padding='post', truncating='pre')\n",
    "X_test_glove_pad = pad_sequences(X_test_glove, \n",
    "                                 maxlen=max_length, dtype='float32',\n",
    "                                 padding='post', truncating='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davideposillipo/Documents/Lavoro/UNIPD_NLP_course_April25/NLP_Unipd/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m117,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,377</span> (458.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m117,377\u001b[0m (458.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,377</span> (458.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m117,377\u001b[0m (458.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# A Simple Model for LSTM\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(units=128, \n",
    "                            input_shape=(max_length, X_train_glove_pad.shape[2]), \n",
    "                            dropout=0, \n",
    "                            recurrent_dropout=0, \n",
    "                            return_sequences=False))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200ms/step - accuracy: 0.5365 - loss: 0.6904 - val_accuracy: 0.5030 - val_loss: 0.6840\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - accuracy: 0.5922 - loss: 0.6562 - val_accuracy: 0.6333 - val_loss: 0.6699\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - accuracy: 0.6755 - loss: 0.6280 - val_accuracy: 0.6774 - val_loss: 0.6192\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 221ms/step - accuracy: 0.7253 - loss: 0.5711 - val_accuracy: 0.7595 - val_loss: 0.5116\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - accuracy: 0.7516 - loss: 0.5351 - val_accuracy: 0.7295 - val_loss: 0.5561\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 193ms/step - accuracy: 0.7750 - loss: 0.4929 - val_accuracy: 0.7635 - val_loss: 0.4882\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 191ms/step - accuracy: 0.7934 - loss: 0.4687 - val_accuracy: 0.7615 - val_loss: 0.5530\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.7867 - loss: 0.5066 - val_accuracy: 0.7956 - val_loss: 0.4919\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - accuracy: 0.8022 - loss: 0.4600 - val_accuracy: 0.7715 - val_loss: 0.4737\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 193ms/step - accuracy: 0.8321 - loss: 0.4043 - val_accuracy: 0.7936 - val_loss: 0.4651\n",
      "\n",
      " Test accuracy =  0.786057710647583\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "epoche=10\n",
    "b_size=128\n",
    "verb=1\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=verb, patience=5)\n",
    "#es=callbacks.ModelCheckpoint(filepath='./nnet_for.hdf5', monitor='val_mean_squared_error', verbose=2, save_best_only=True) # con questo fa tutte le epoche ma salva il migliore. SOpra può fermarsi prima di fine epoche\n",
    "history=model.fit(X_train_glove_pad,Y_train,\n",
    "\t\t\t\t\tepochs=epoche,\n",
    "\t\t\t\t\tvalidation_data=(X_val_glove_pad,Y_val),\n",
    "\t\t\t\t\tbatch_size=b_size,\n",
    "\t\t\t\t\tcallbacks=[es],\n",
    "\t\t\t\t\tverbose=verb)\n",
    "\n",
    "print('\\n Test accuracy = ', model.evaluate(X_test_glove_pad,Y_test, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX: alternative syntax - Embedding with Glove and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "X_train_tkn = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_val_tkn = word_tokenizer.texts_to_sequences(X_val)\n",
    "X_test_tkn = word_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# Padding all reviews to fixed length 100\n",
    "maxlen = max_length\n",
    "X_train_tkn_pad = pad_sequences(X_train_tkn, padding='post', truncating='pre', maxlen=maxlen)\n",
    "X_val_tkn_pad = pad_sequences(X_val_tkn, padding='post', truncating='pre', maxlen=maxlen)\n",
    "X_test_tkn_pad = pad_sequences(X_test_tkn, padding='post', truncating='pre', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('../../data/glove/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length = len(glove_words)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Print Embedding Matrix shape\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davideposillipo/Documents/Lavoro/UNIPD_NLP_course_April25/NLP_Unipd/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">40,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │    \u001b[38;5;34m40,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,000,000</span> (152.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m40,000,000\u001b[0m (152.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,000,000</span> (152.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m40,000,000\u001b[0m (152.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204ms/step - acc: 0.4885 - loss: 0.6934 - val_acc: 0.5439 - val_loss: 0.6870\n",
      "Epoch 2/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - acc: 0.5429 - loss: 0.6768 - val_acc: 0.5489 - val_loss: 0.6771\n",
      "Epoch 3/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - acc: 0.5847 - loss: 0.6564 - val_acc: 0.5539 - val_loss: 0.6953\n",
      "Epoch 4/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - acc: 0.5951 - loss: 0.6574 - val_acc: 0.5464 - val_loss: 0.6821\n",
      "Epoch 5/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 210ms/step - acc: 0.5997 - loss: 0.6504 - val_acc: 0.5714 - val_loss: 0.6729\n",
      "Epoch 6/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - acc: 0.6226 - loss: 0.6446 - val_acc: 0.6917 - val_loss: 0.6377\n",
      "Epoch 7/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - acc: 0.7076 - loss: 0.5987 - val_acc: 0.7018 - val_loss: 0.6175\n",
      "Epoch 8/8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 209ms/step - acc: 0.7425 - loss: 0.5693 - val_acc: 0.6892 - val_loss: 0.6094\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Neural Network architecture\n",
    "lstm_model = keras.models.Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(128))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Model compiling\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(lstm_model.summary())\n",
    "\n",
    "# Model Training\n",
    "lstm_model_history = lstm_model.fit(X_train_tkn_pad, Y_train, batch_size=128, epochs=8, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy =  0.6875\n"
     ]
    }
   ],
   "source": [
    "print('\\n Test accuracy = ', lstm_model.evaluate(X_test_tkn_pad,Y_test, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e18c37fd15a1974bcbd8e3537474f367f1f4a053e6ef9eedc221dd433503edc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
