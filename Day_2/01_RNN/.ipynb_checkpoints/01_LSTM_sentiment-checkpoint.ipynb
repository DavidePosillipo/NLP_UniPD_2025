{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Sentiment classification\n",
    "\n",
    "Using a subset of the aclImdb dataset this notebook builds and trains a text classification engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS USED TO CREATE A SUBSET OF THE WHOLE aclImdb DATASET\n",
    "# If the data/aclImdb_subset directory exists this cell does nothing.\n",
    "# SET THE VARIABLES IN THE MIDDLE OF THE CELL to create each subfolder.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "if not os.path.isdir(\"../../Data/aclImdb_subset/\"):\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/train/pos\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/train/neg\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/test/pos\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/test/neg\")))\n",
    "\n",
    "    train_pos_files = os.listdir(\"../../Data/aclImdb/train/pos\")\n",
    "    train_neg_files = os.listdir(\"../../Data/aclImdb/train/neg\")\n",
    "    test_pos_files = os.listdir(\"../../Data/aclImdb/test/pos\")\n",
    "    test_neg_files = os.listdir(\"../../Data/aclImdb/test/neg\")\n",
    "\n",
    "\n",
    "    # SET THESE 3 VARIABLES\n",
    "    train_or_test = 'test'\n",
    "    pos_or_neg = 'neg'\n",
    "    file_names = test_neg_files\n",
    "    ############################\n",
    "\n",
    "    in_folder = \"../../Data/aclImdb/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "    out_folder = \"../../Data/aclImdb_subset/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "\n",
    "    used_indexes = []\n",
    "    for i in range(int(len(os.listdir(in_folder))/10)):\n",
    "        index = random.randint(0, len(os.listdir(in_folder)))\n",
    "        while index in used_indexes:\n",
    "            index = random.randint(0, len(os.listdir(in_folder)))\n",
    "        file_ = in_folder + file_names[index]\n",
    "        shutil.copy(file_ , out_folder)\n",
    "        used_indexes.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from tensorflow import keras\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk_stopw = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Text Corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = \"../../Data/aclImdb_subset/\"\n",
    "labelToName = { 0 : 'neg', 1: 'pos' }\n",
    "def getMovies(split):\n",
    "    '''\n",
    "    outputs:\n",
    "    X_raw: list of reviews\n",
    "    Y: target array; len(Y)=len(X_raw)\n",
    "    '''\n",
    "    X_raw, Y  = [], []\n",
    "\n",
    "    for classIndex, directory in enumerate(['neg', 'pos']):\n",
    "        dirName = data + split + \"/\" + directory\n",
    "        for reviewFile in os.listdir(dirName):\n",
    "            with open (dirName + '/' + reviewFile, 'r', encoding='utf8') as f:\n",
    "                raw = f.read()\n",
    "                if (len(raw) == 0):\n",
    "                    continue\n",
    "            X_raw.append(raw)\n",
    "            Y.append(classIndex)\n",
    "    return X_raw, np.array(Y)\n",
    "\n",
    "# We will split later in train and val\n",
    "X_raw, Y = getMovies(split='train')\n",
    "\n",
    "X_raw_test, Y_test = getMovies(split='test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_char_train = [len(x) for x in X_raw]\n",
    "n_char_test = [len(x) for x in X_raw_test]\n",
    "print('TRAIN: ', len(X_raw),' reviews; ','minimum length = ', min(n_char_train), ', max length = ',max(n_char_train), ', median',np.median(n_char_train), 'characters')\n",
    "print('TEST: ', len(X_raw_test),' reviews; ','minimum length = ', min(n_char_test), ', max length = ',max(n_char_test), ', median',np.median(n_char_test), 'characters')\n",
    "\n",
    "print('\\n \\n TEXT \\n',X_raw[0],'\\n LABEL =', labelToName[Y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "lowcase, tokenize, remove punctuations, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(pos):\n",
    "    '''\n",
    "    Convert nltk.pos_tag() tags  so that they can be understood by pos tags by nltk.WordNetLemmatizer()\n",
    "    '''\n",
    "    if pos.startswith('J'):\n",
    "        return 'a' # o wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return 'v' # o wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return 'n' # o wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return 'r' # o wordnet.ADV\n",
    "    else:          \n",
    "        return 'n' # default \n",
    "\n",
    "def txt_preprocessing(X, printa=False):\n",
    "    i = 0 #text to print\n",
    "    #lowcase\n",
    "    X = [x.lower() for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # tokenize: token are made of strings or of alphanumerical strings; punctuaction and special chars are excluded.\n",
    "    # token with <=2 or >14 chars are removed\n",
    "    X = [RegexpTokenizer(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b').tokenize(x) for x in X] # or [re.findall(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b',x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    #remove stop words\n",
    "    X = [(lambda x: [x_i for x_i in x if x_i not in nltk_stopw])(x) for x in X] # alternatively list(map(lambda x: ([x_i for x_i in x if x_i not in nltk_stopw]),X))\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatization using POS\n",
    "    X = [nltk.pos_tag(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # map POS tags to work with nltk.WordNetLemmatizer()\n",
    "    X = [ (lambda x: [(x_i[0],get_pos(x_i[1])) for x_i in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatize\n",
    "    X = [(lambda x: [nltk.WordNetLemmatizer().lemmatize(w,p) for w,p in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # reshape as a list of sentences: [['this','is','string','1'], ['this','is','string','2']...] --> ['this is string 1','this is string 2'...]\n",
    "    X = [\" \".join(x) for x in X]\n",
    "    if printa: print(X[i])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all passages in txt_processing\n",
    "txt_preprocessing([X_raw[0]], printa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 minute run\n",
    "X = txt_preprocessing(X_raw)\n",
    "X_test = txt_preprocessing(X_raw_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_raw[0],'\\n\\n',X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/Train Split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2, random_state=123)\n",
    "print(Y_train.mean(), Y_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until the cell above the code is in common for setions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent model with sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "sequence_length = max_length\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    output_mode=\"int\", # every token is assigned an index\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "vocab_size = vectorize_layer.vocabulary_size()\n",
    "print('distinct tokens (including 1 for UNK and 0 for padded tokens) = ', vectorize_layer.vocabulary_size())\n",
    "\n",
    "X_train_encoded = vectorize_layer(X_train)\n",
    "X_val_encoded = vectorize_layer(X_val)\n",
    "X_test_encoded = vectorize_layer(X_test)\n",
    "\n",
    "print('X_train_encoded.shape  = ',X_train_encoded.shape)\n",
    "print('X_val_encoded.shape  = ',X_val_encoded.shape)\n",
    "print('X_test_encoded.shape  = ',X_test_encoded.shape)\n",
    "\n",
    "print('\\n  = ',X_train_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Simple Model for LSTM\n",
    "model = keras.models.Sequential()\n",
    "# number of params of the embedding layer is vocab_size*output_dim\n",
    "embedding = keras.layers.Embedding(  input_dim= vocab_size,\n",
    "                                                output_dim=50, # 128\n",
    "                                                input_length=sequence_length, trainable=True ,mask_zero=True) #maskzero -_> sa che 0 è speciale e non avrà 'peso' nella loss\n",
    "model.add(embedding)\n",
    "model.add(keras.layers.LSTM(units=150, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model (up to 10 min)\n",
    "epoche=10\n",
    "b_size=32\n",
    "verb=1\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=verb, patience=5)\n",
    "#es=callbacks.ModelCheckpoint(filepath='./nnet_for.hdf5', monitor='val_mean_squared_error', verbose=2, save_best_only=True) # con questo fa tutte le epoche ma salva il migliore. SOpra può fermarsi prima di fine epoche\n",
    "history=model.fit(X_train_encoded,Y_train,\n",
    "\t\t\t\t\tepochs=epoche,\n",
    "\t\t\t\t\tvalidation_data=(X_val_encoded,Y_val),\n",
    "\t\t\t\t\tbatch_size=b_size,\n",
    "\t\t\t\t\tcallbacks=[es],\n",
    "\t\t\t\t\tverbose=verb)\n",
    "\n",
    "print('\\n Test accuracy = ', model.evaluate(X_test_encoded,Y_test, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e18c37fd15a1974bcbd8e3537474f367f1f4a053e6ef9eedc221dd433503edc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
