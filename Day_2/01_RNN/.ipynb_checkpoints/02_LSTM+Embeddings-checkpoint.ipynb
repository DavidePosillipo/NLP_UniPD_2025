{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification with word embeddings\n",
    "\n",
    "Words are different from images or even molecules, in that the meaning of a word is not represented by the letters that make up the word (the same way that the meaning of an image is represented by the pixels that make up the pixel).  \n",
    "Instead, <b>the meaning of words comes from how they are used in conjunction with other words.</b>  \n",
    "\n",
    "### GloVe, Global Vectors for Word Representation\n",
    "\n",
    "There are multiple versions of pre-trained GloVe word embeddings.  \n",
    "They differ in the <i>corpus</i> used to train the embedding, and the <i>size</i> of the embeddings.\n",
    "\n",
    "GloVe is a project Stanford NLP: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS USED TO CREATE A SUBSET OF THE WHOLE aclImdb DATASET\n",
    "# SET THE VARIABLES IN THE MIDDLE OF THE CELL!!!\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "ACTIVATE_CODE = False\n",
    "\n",
    "if ACTIVATE_CODE:\n",
    "    if not os.path.isdir(\"../Data/aclImdb_subset/\"):\n",
    "        print(len(os.listdir(\"../Data/aclImdb/train/pos\")))\n",
    "        print(len(os.listdir(\"../Data/aclImdb/train/neg\")))\n",
    "        print(len(os.listdir(\"../Data/aclImdb/test/pos\")))\n",
    "        print(len(os.listdir(\"../Data/aclImdb/test/neg\")))\n",
    "\n",
    "        train_pos_files = os.listdir(\"../Data/aclImdb/train/pos\")\n",
    "        train_neg_files = os.listdir(\"../Data/aclImdb/train/neg\")\n",
    "        test_pos_files = os.listdir(\"../Data/aclImdb/test/pos\")\n",
    "        test_neg_files = os.listdir(\"../Data/aclImdb/test/neg\")\n",
    "\n",
    "\n",
    "        # SET THESE 3 VARIABLES\n",
    "        train_or_test = 'test'\n",
    "        pos_or_neg = 'neg'\n",
    "        file_names = test_neg_files\n",
    "        ############################\n",
    "\n",
    "        in_folder = \"../Data/aclImdb/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "        out_folder = \"../Data/aclImdb_subset/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "\n",
    "        used_indexes = []\n",
    "        for i in range(int(len(os.listdir(in_folder))/10)):\n",
    "            index = random.randint(0, len(os.listdir(in_folder)))\n",
    "            while index in used_indexes:\n",
    "                index = random.randint(0, len(os.listdir(in_folder)))\n",
    "            file_ = in_folder + file_names[index]\n",
    "            shutil.copy(file_ , out_folder)\n",
    "            used_indexes.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from tensorflow import keras\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk_stopw = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Text Corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = \"../Data/aclImdb_subset/\"\n",
    "labelToName = { 0 : 'neg', 1: 'pos' }\n",
    "def getMovies(split):\n",
    "    '''\n",
    "    outputs:\n",
    "    X_raw: lista di  recensioni\n",
    "    Y: array di target; len(Y)=len(X_raw)\n",
    "    '''\n",
    "    X_raw, Y  = [], []\n",
    "\n",
    "    for classIndex, directory in enumerate(['neg', 'pos']):\n",
    "        dirName = data + split + \"/\" + directory\n",
    "        for reviewFile in os.listdir(dirName):\n",
    "            with open (dirName + '/' + reviewFile, 'r', encoding='utf8') as f:\n",
    "                raw = f.read()\n",
    "                if (len(raw) == 0):\n",
    "                    continue\n",
    "            X_raw.append(raw)\n",
    "            Y.append(classIndex)\n",
    "    return X_raw, np.array(Y)\n",
    "\n",
    "# We will split later in train and val\n",
    "X_raw, Y = getMovies(split='train')\n",
    "\n",
    "X_raw_test, Y_test = getMovies(split='test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_char_train = [len(x) for x in X_raw]\n",
    "n_char_test = [len(x) for x in X_raw_test]\n",
    "print('TRAIN: ', len(X_raw),' reviews; ','min length = ', min(n_char_train), ', max length = ',max(n_char_train), ', median',np.median(n_char_train), 'chars')\n",
    "print('TEST: ', len(X_raw_test),' reviews; ','min length = ', min(n_char_test), ', mac length = ',max(n_char_test), ', median',np.median(n_char_test), 'chars')\n",
    "\n",
    "print('\\n \\n TEXT \\n',X_raw[0],'\\n LABEL =', labelToName[Y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "lowcase, tokenize, remove punctuations, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(pos):\n",
    "    '''\n",
    "    Convert nltk.pos_tag() tags  so that they can be understood by pos tags by nltk.WordNetLemmatizer()\n",
    "    '''\n",
    "    if pos.startswith('J'):\n",
    "        return 'a' # o wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return 'v' # o wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return 'n' # o wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return 'r' # o wordnet.ADV\n",
    "    else:          \n",
    "        return 'n' # default  \n",
    "\n",
    "def txt_preprocessing(X, printa=False):\n",
    "    i = 0 #text to print\n",
    "    #lowcase\n",
    "    X = [x.lower() for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # tokenize\n",
    "    X = [RegexpTokenizer(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b').tokenize(x) for x in X] #or [re.findall(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b',x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # remove stop words\n",
    "    X = [(lambda x: [x_i for x_i in x if x_i not in nltk_stopw])(x) for x in X] # or list(map(lambda x: ([x_i for x_i in x if x_i not in nltk_stopw]),X))\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatization using POS\n",
    "    X = [nltk.pos_tag(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # POS tags to match nltk.WordNetLemmatizer()\n",
    "    X = [ (lambda x: [(x_i[0],get_pos(x_i[1])) for x_i in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatizzo\n",
    "    X = [(lambda x: [nltk.WordNetLemmatizer().lemmatize(w,p) for w,p in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # reshape as a list of sentences: [['this','is','string','1'], ['this','is','string','2']...] --> ['this is string 1','this is string 2'...]\n",
    "    X = [\" \".join(x) for x in X]\n",
    "    if printa: print(X[i])\n",
    "\n",
    "    return X\n",
    "\n",
    "#a=['thIs Film was#@ the ?worst Ever', 'I sAw,  !very good Films recently!']    \n",
    "#txt_preprocessing(a, printa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all passages in txt_processing\n",
    "txt_preprocessing([X_raw[0]], printa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 minute run\n",
    "X = txt_preprocessing(X_raw)\n",
    "X_test = txt_preprocessing(X_raw_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_raw[0],'\\n\\n',X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test/Train Split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2, random_state=123)\n",
    "print(Y_train.mean(), Y_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with Glove. \n",
    "If not present, browse to https://nlp.stanford.edu/projects/glove/ and download glove.6B.zip.\n",
    "Unzip files and put in a new directory \"glove\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r',encoding=\"utf8\") as f:\n",
    "\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "    return words, word_to_vec_map\n",
    "\n",
    "\n",
    "# Load the GloVe word embeddings file\n",
    "glove_file = \"../Data/glove/glove.6B.100d.txt\"\n",
    "glove_words, glove_word2vec_map = read_glove_vecs(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vectorize_sentence(sentence):\n",
    "    lemmas = sentence.split()\n",
    "    vectors = []\n",
    "    for word in lemmas:\n",
    "        if word in glove_words:\n",
    "            vectors.append(glove_word2vec_map[word])\n",
    "        else:\n",
    "            vectors.append(glove_word2vec_map[\"unk\"])\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = [glove_vectorize_sentence(sentence) for sentence in X_train]\n",
    "X_val_glove = [glove_vectorize_sentence(sentence) for sentence in X_val]\n",
    "X_test_glove = [glove_vectorize_sentence(sentence) for sentence in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad to take all sequences to same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "lengths = [len(vecs) for vecs in X_train_glove]\n",
    "max_length = int(np.percentile(lengths, 70))  # 80th percentile\n",
    "print(\"max_length is: \", max_length)\n",
    "    \n",
    "X_train_glove_pad = pad_sequences(X_train_glove,\n",
    "                                  maxlen=max_length, dtype='float32',\n",
    "                                  padding='post', truncating='pre')\n",
    "X_val_glove_pad = pad_sequences(X_val_glove,\n",
    "                                maxlen=max_length,\n",
    "                                dtype='float32', padding='post', truncating='pre')\n",
    "X_test_glove_pad = pad_sequences(X_test_glove, \n",
    "                                 maxlen=max_length, dtype='float32',\n",
    "                                 padding='post', truncating='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Simple Model for LSTM\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(units=128, \n",
    "                            input_shape=(max_length, X_train_glove_pad.shape[2]), \n",
    "                            dropout=0, \n",
    "                            recurrent_dropout=0, \n",
    "                            return_sequences=False))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "epoche=10\n",
    "b_size=128\n",
    "verb=1\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=verb, patience=5)\n",
    "#es=callbacks.ModelCheckpoint(filepath='./nnet_for.hdf5', monitor='val_mean_squared_error', verbose=2, save_best_only=True) # con questo fa tutte le epoche ma salva il migliore. SOpra pu√≤ fermarsi prima di fine epoche\n",
    "history=model.fit(X_train_glove_pad,Y_train,\n",
    "\t\t\t\t\tepochs=epoche,\n",
    "\t\t\t\t\tvalidation_data=(X_val_glove_pad,Y_val),\n",
    "\t\t\t\t\tbatch_size=b_size,\n",
    "\t\t\t\t\tcallbacks=[es],\n",
    "\t\t\t\t\tverbose=verb)\n",
    "\n",
    "print('\\n Test accuracy = ', model.evaluate(X_test_glove_pad,Y_test, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX: alternative syntax - Embedding with Glove and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "X_train_tkn = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_val_tkn = word_tokenizer.texts_to_sequences(X_val)\n",
    "X_test_tkn = word_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# Padding all reviews to fixed length 100\n",
    "maxlen = max_length\n",
    "X_train_tkn_pad = pad_sequences(X_train_tkn, padding='post', truncating='pre', maxlen=maxlen)\n",
    "X_val_tkn_pad = pad_sequences(X_val_tkn, padding='post', truncating='pre', maxlen=maxlen)\n",
    "X_test_tkn_pad = pad_sequences(X_test_tkn, padding='post', truncating='pre', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('../Data/glove/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(glove_words)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Print Embedding Matrix shape\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Neural Network architecture\n",
    "lstm_model = keras.models.Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(128))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Model compiling\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(lstm_model.summary())\n",
    "\n",
    "# Model Training\n",
    "lstm_model_history = lstm_model.fit(X_train_tkn_pad, Y_train, batch_size=128, epochs=8, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e18c37fd15a1974bcbd8e3537474f367f1f4a053e6ef9eedc221dd433503edc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
