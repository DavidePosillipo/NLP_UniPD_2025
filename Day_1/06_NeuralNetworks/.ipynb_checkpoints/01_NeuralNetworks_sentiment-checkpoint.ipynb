{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment classification with Neural Network\n",
    "\n",
    "Using a subset of the aclImdb dataset this notebook builds and trains a text classification engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS USED TO CREATE A SUBSET OF THE WHOLE aclImdb DATASET\n",
    "# If the data/aclImdb_subset directory exists this cell does nothing.\n",
    "# SET THE VARIABLES IN THE MIDDLE OF THE CELL to create each subfolder.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "if not os.path.isdir(\"../../Data/aclImdb_subset/\"):\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/train/pos\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/train/neg\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/test/pos\")))\n",
    "    print(len(os.listdir(\"../../Data/aclImdb/test/neg\")))\n",
    "\n",
    "    train_pos_files = os.listdir(\"../../Data/aclImdb/train/pos\")\n",
    "    train_neg_files = os.listdir(\"../../Data/aclImdb/train/neg\")\n",
    "    test_pos_files = os.listdir(\"../../Data/aclImdb/test/pos\")\n",
    "    test_neg_files = os.listdir(\"../../Data/aclImdb/test/neg\")\n",
    "\n",
    "\n",
    "    # SET THESE 3 VARIABLES\n",
    "    train_or_test = 'test'\n",
    "    pos_or_neg = 'neg'\n",
    "    file_names = test_neg_files\n",
    "    ############################\n",
    "\n",
    "    in_folder = \"../../Data/aclImdb/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "    out_folder = \"../../Data/aclImdb_subset/\" + train_or_test + \"/\" + pos_or_neg + \"/\"\n",
    "\n",
    "    used_indexes = []\n",
    "    for i in range(int(len(os.listdir(in_folder))/10)):\n",
    "        index = random.randint(0, len(os.listdir(in_folder)))\n",
    "        while index in used_indexes:\n",
    "            index = random.randint(0, len(os.listdir(in_folder)))\n",
    "        file_ = in_folder + file_names[index]\n",
    "        shutil.copy(file_ , out_folder)\n",
    "        used_indexes.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from tensorflow import keras\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk_stopw = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Read the Text Corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = \"../../Data/aclImdb_subset/\"\n",
    "labelToName = { 0 : 'neg', 1: 'pos' }\n",
    "def getMovies(split):\n",
    "    '''\n",
    "    outputs:\n",
    "    X_raw: list of reviews\n",
    "    Y: target array; len(Y)=len(X_raw)\n",
    "    '''\n",
    "    X_raw, Y  = [], []\n",
    "\n",
    "    for classIndex, directory in enumerate(['neg', 'pos']):\n",
    "        dirName = data + split + \"/\" + directory\n",
    "        for reviewFile in os.listdir(dirName):\n",
    "            with open (dirName + '/' + reviewFile, 'r', encoding='utf8') as f:\n",
    "                raw = f.read()\n",
    "                if (len(raw) == 0):\n",
    "                    continue\n",
    "            X_raw.append(raw)\n",
    "            Y.append(classIndex)\n",
    "    return X_raw, np.array(Y)\n",
    "\n",
    "# We will split later in train and val\n",
    "X_raw, Y = getMovies(split='train')\n",
    "\n",
    "X_raw_test, Y_test = getMovies(split='test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  2494  reviews;  minimum length =  81 , max length =  8969 , median 975.0 characters\n",
      "TEST:  2496  reviews;  minimum length =  32 , max length =  12988 , median 961.0 characters\n",
      "\n",
      " \n",
      " TEXT \n",
      " A young scientist is trying to carry on his dead father's work on limb regeneration.His overbearing mother has convinced him that he murdered his own father and is monitoring his progress for her own evil purposes.A young doctor uses reptilian DNA he extracts from a large creature and when his arm is conveniently ripped off a few minutes later,he injects himself with his formula and grows a new murderous arm...Admittedly the special effects in \"Severed Ties\" are pretty good and grotesque,but the rest of the film is awful.The severed arm is behaving like a snake and kills few people.Big deal.The acting is mediocre and the climax is silly.3 out of 10. \n",
      " LABEL = neg\n"
     ]
    }
   ],
   "source": [
    "n_char_train = [len(x) for x in X_raw]\n",
    "n_char_test = [len(x) for x in X_raw_test]\n",
    "print('TRAIN: ', len(X_raw),' reviews; ','minimum length = ', min(n_char_train), ', max length = ',max(n_char_train), ', median',np.median(n_char_train), 'characters')\n",
    "print('TEST: ', len(X_raw_test),' reviews; ','minimum length = ', min(n_char_test), ', max length = ',max(n_char_test), ', median',np.median(n_char_test), 'characters')\n",
    "\n",
    "print('\\n \\n TEXT \\n',X_raw[0],'\\n LABEL =', labelToName[Y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "lowcase, tokenize, remove punctuations, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(pos):\n",
    "    '''\n",
    "    Convert nltk.pos_tag() tags  so that they can be understood by pos tags by nltk.WordNetLemmatizer()\n",
    "    '''\n",
    "    if pos.startswith('J'):\n",
    "        return 'a' # o wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return 'v' # o wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return 'n' # o wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return 'r' # o wordnet.ADV\n",
    "    else:          \n",
    "        return 'n' # default \n",
    "\n",
    "def txt_preprocessing(X, printa=False):\n",
    "    i = 0 #text to print\n",
    "    #lowcase\n",
    "    X = [x.lower() for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # tokenize: token are made of strings or of alphanumerical strings; punctuaction and special chars are excluded.\n",
    "    # token with <=2 or >14 chars are removed\n",
    "    X = [RegexpTokenizer(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b').tokenize(x) for x in X] # or [re.findall(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b',x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    #remove stop words\n",
    "    X = [(lambda x: [x_i for x_i in x if x_i not in nltk_stopw])(x) for x in X] # alternatively list(map(lambda x: ([x_i for x_i in x if x_i not in nltk_stopw]),X))\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatization using POS\n",
    "    X = [nltk.pos_tag(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # map POS tags to work with nltk.WordNetLemmatizer()\n",
    "    X = [ (lambda x: [(x_i[0],get_pos(x_i[1])) for x_i in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # lemmatize\n",
    "    X = [(lambda x: [nltk.WordNetLemmatizer().lemmatize(w,p) for w,p in x])(x) for x in X]\n",
    "    if printa: print(X[i],'\\n')\n",
    "\n",
    "    # reshape as a list of sentences: [['this','is','string','1'], ['this','is','string','2']...] --> ['this is string 1','this is string 2'...]\n",
    "    X = [\" \".join(x) for x in X]\n",
    "    if printa: print(X[i])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a young scientist is trying to carry on his dead father's work on limb regeneration.his overbearing mother has convinced him that he murdered his own father and is monitoring his progress for her own evil purposes.a young doctor uses reptilian dna he extracts from a large creature and when his arm is conveniently ripped off a few minutes later,he injects himself with his formula and grows a new murderous arm...admittedly the special effects in \"severed ties\" are pretty good and grotesque,but the rest of the film is awful.the severed arm is behaving like a snake and kills few people.big deal.the acting is mediocre and the climax is silly.3 out of 10. \n",
      "\n",
      "['young', 'scientist', 'trying', 'carry', 'his', 'dead', 'father', 'work', 'limb', 'regeneration', 'his', 'overbearing', 'mother', 'has', 'convinced', 'him', 'that', 'murdered', 'his', 'own', 'father', 'and', 'monitoring', 'his', 'progress', 'for', 'her', 'own', 'evil', 'purposes', 'young', 'doctor', 'uses', 'reptilian', 'dna', 'extracts', 'from', 'large', 'creature', 'and', 'when', 'his', 'arm', 'conveniently', 'ripped', 'off', 'few', 'minutes', 'later', 'injects', 'himself', 'with', 'his', 'formula', 'and', 'grows', 'new', 'murderous', 'arm', 'admittedly', 'the', 'special', 'effects', 'severed', 'ties', 'are', 'pretty', 'good', 'and', 'grotesque', 'but', 'the', 'rest', 'the', 'film', 'awful', 'the', 'severed', 'arm', 'behaving', 'like', 'snake', 'and', 'kills', 'few', 'people', 'big', 'deal', 'the', 'acting', 'mediocre', 'and', 'the', 'climax', 'silly', 'out'] \n",
      "\n",
      "['young', 'scientist', 'trying', 'carry', 'dead', 'father', 'work', 'limb', 'regeneration', 'overbearing', 'mother', 'convinced', 'murdered', 'father', 'monitoring', 'progress', 'evil', 'purposes', 'young', 'doctor', 'uses', 'reptilian', 'dna', 'extracts', 'large', 'creature', 'arm', 'conveniently', 'ripped', 'minutes', 'later', 'injects', 'formula', 'grows', 'new', 'murderous', 'arm', 'admittedly', 'special', 'effects', 'severed', 'ties', 'pretty', 'good', 'grotesque', 'rest', 'film', 'awful', 'severed', 'arm', 'behaving', 'like', 'snake', 'kills', 'people', 'big', 'deal', 'acting', 'mediocre', 'climax', 'silly'] \n",
      "\n",
      "[('young', 'JJ'), ('scientist', 'NN'), ('trying', 'VBG'), ('carry', 'NN'), ('dead', 'JJ'), ('father', 'NN'), ('work', 'NN'), ('limb', 'NN'), ('regeneration', 'NN'), ('overbearing', 'VBG'), ('mother', 'NN'), ('convinced', 'VBN'), ('murdered', 'VBD'), ('father', 'RBR'), ('monitoring', 'NN'), ('progress', 'NN'), ('evil', 'JJ'), ('purposes', 'NNS'), ('young', 'JJ'), ('doctor', 'NN'), ('uses', 'VBZ'), ('reptilian', 'JJ'), ('dna', 'NN'), ('extracts', 'NNS'), ('large', 'JJ'), ('creature', 'NN'), ('arm', 'NN'), ('conveniently', 'RB'), ('ripped', 'VBN'), ('minutes', 'NNS'), ('later', 'RBR'), ('injects', 'NNS'), ('formula', 'VBP'), ('grows', 'VBZ'), ('new', 'JJ'), ('murderous', 'JJ'), ('arm', 'NN'), ('admittedly', 'RB'), ('special', 'JJ'), ('effects', 'NNS'), ('severed', 'VBD'), ('ties', 'NNS'), ('pretty', 'RB'), ('good', 'JJ'), ('grotesque', 'NN'), ('rest', 'NN'), ('film', 'NN'), ('awful', 'JJ'), ('severed', 'VBD'), ('arm', 'JJ'), ('behaving', 'NN'), ('like', 'IN'), ('snake', 'NN'), ('kills', 'NNS'), ('people', 'NNS'), ('big', 'JJ'), ('deal', 'NN'), ('acting', 'VBG'), ('mediocre', 'NN'), ('climax', 'NN'), ('silly', 'RB')] \n",
      "\n",
      "[('young', 'a'), ('scientist', 'n'), ('trying', 'v'), ('carry', 'n'), ('dead', 'a'), ('father', 'n'), ('work', 'n'), ('limb', 'n'), ('regeneration', 'n'), ('overbearing', 'v'), ('mother', 'n'), ('convinced', 'v'), ('murdered', 'v'), ('father', 'r'), ('monitoring', 'n'), ('progress', 'n'), ('evil', 'a'), ('purposes', 'n'), ('young', 'a'), ('doctor', 'n'), ('uses', 'v'), ('reptilian', 'a'), ('dna', 'n'), ('extracts', 'n'), ('large', 'a'), ('creature', 'n'), ('arm', 'n'), ('conveniently', 'r'), ('ripped', 'v'), ('minutes', 'n'), ('later', 'r'), ('injects', 'n'), ('formula', 'v'), ('grows', 'v'), ('new', 'a'), ('murderous', 'a'), ('arm', 'n'), ('admittedly', 'r'), ('special', 'a'), ('effects', 'n'), ('severed', 'v'), ('ties', 'n'), ('pretty', 'r'), ('good', 'a'), ('grotesque', 'n'), ('rest', 'n'), ('film', 'n'), ('awful', 'a'), ('severed', 'v'), ('arm', 'a'), ('behaving', 'n'), ('like', 'n'), ('snake', 'n'), ('kills', 'n'), ('people', 'n'), ('big', 'a'), ('deal', 'n'), ('acting', 'v'), ('mediocre', 'n'), ('climax', 'n'), ('silly', 'r')] \n",
      "\n",
      "['young', 'scientist', 'try', 'carry', 'dead', 'father', 'work', 'limb', 'regeneration', 'overbear', 'mother', 'convince', 'murder', 'father', 'monitoring', 'progress', 'evil', 'purpose', 'young', 'doctor', 'use', 'reptilian', 'dna', 'extract', 'large', 'creature', 'arm', 'conveniently', 'rip', 'minute', 'later', 'injects', 'formula', 'grow', 'new', 'murderous', 'arm', 'admittedly', 'special', 'effect', 'sever', 'tie', 'pretty', 'good', 'grotesque', 'rest', 'film', 'awful', 'sever', 'arm', 'behaving', 'like', 'snake', 'kill', 'people', 'big', 'deal', 'act', 'mediocre', 'climax', 'silly'] \n",
      "\n",
      "young scientist try carry dead father work limb regeneration overbear mother convince murder father monitoring progress evil purpose young doctor use reptilian dna extract large creature arm conveniently rip minute later injects formula grow new murderous arm admittedly special effect sever tie pretty good grotesque rest film awful sever arm behaving like snake kill people big deal act mediocre climax silly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['young scientist try carry dead father work limb regeneration overbear mother convince murder father monitoring progress evil purpose young doctor use reptilian dna extract large creature arm conveniently rip minute later injects formula grow new murderous arm admittedly special effect sever tie pretty good grotesque rest film awful sever arm behaving like snake kill people big deal act mediocre climax silly']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all passages in txt_processing\n",
    "txt_preprocessing([X_raw[0]], printa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 minute run\n",
    "X = txt_preprocessing(X_raw)\n",
    "X_test = txt_preprocessing(X_raw_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A young scientist is trying to carry on his dead father's work on limb regeneration.His overbearing mother has convinced him that he murdered his own father and is monitoring his progress for her own evil purposes.A young doctor uses reptilian DNA he extracts from a large creature and when his arm is conveniently ripped off a few minutes later,he injects himself with his formula and grows a new murderous arm...Admittedly the special effects in \"Severed Ties\" are pretty good and grotesque,but the rest of the film is awful.The severed arm is behaving like a snake and kills few people.Big deal.The acting is mediocre and the climax is silly.3 out of 10. \n",
      "\n",
      " young scientist try carry dead father work limb regeneration overbear mother convince murder father monitoring progress evil purpose young doctor use reptilian dna extract large creature arm conveniently rip minute later injects formula grow new murderous arm admittedly special effect sever tie pretty good grotesque rest film awful sever arm behaving like snake kill people big deal act mediocre climax silly\n"
     ]
    }
   ],
   "source": [
    "print(X_raw[0],'\\n\\n',X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4907268170426065 0.5390781563126252\n"
     ]
    }
   ],
   "source": [
    "# Test/Train Split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2, random_state=123)\n",
    "print(Y_train.mean(), Y_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until the cell above the code is in common for setions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic model with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (1995, 20431)\n",
      "vocab length =  20431\n"
     ]
    }
   ],
   "source": [
    "# Build tf-Idf vectors with sklearn\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit(X_train)\n",
    "X_train_vect = vectorizer.transform(X_train).toarray()\n",
    "X_val_vect = vectorizer.transform(X_val).toarray()\n",
    "X_test_vect = vectorizer.transform(X_test).toarray()\n",
    "print('X_train shape', X_train_vect.shape)\n",
    "print('vocab length = ', len(vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davideposillipo/Documents/Lavoro/UNIPD_NLP_course_April25/NLP_Unipd/.venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">326,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │       \u001b[38;5;34m326,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,929</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m326,929\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,929</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m326,929\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# define a Simple Model \n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(X_train_vect.shape[1],)))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))# oppure 2 e softmax, e sotto sparse_categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5982 - loss: 0.6869 - val_accuracy: 0.8417 - val_loss: 0.6515\n",
      "Epoch 2/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9480 - loss: 0.6019 - val_accuracy: 0.8537 - val_loss: 0.5898\n",
      "Epoch 3/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9653 - loss: 0.4806 - val_accuracy: 0.8477 - val_loss: 0.5259\n",
      "Epoch 4/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9786 - loss: 0.3628 - val_accuracy: 0.8617 - val_loss: 0.4715\n",
      "Epoch 5/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.2633 - val_accuracy: 0.8557 - val_loss: 0.4347\n",
      "Epoch 6/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9962 - loss: 0.1985 - val_accuracy: 0.8497 - val_loss: 0.4112\n",
      "Epoch 7/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9951 - loss: 0.1488 - val_accuracy: 0.8497 - val_loss: 0.3951\n",
      "Epoch 8/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9977 - loss: 0.1126 - val_accuracy: 0.8457 - val_loss: 0.3849\n",
      "Epoch 9/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0854 - val_accuracy: 0.8437 - val_loss: 0.3772\n",
      "Epoch 9: early stopping\n",
      "\n",
      " Test accuracy =  0.8257211446762085\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "epoche=10\n",
    "b_size=32\n",
    "verb=1\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=verb, patience=5)\n",
    "#es=callbacks.ModelCheckpoint(filepath='./nnet_for.hdf5', monitor='val_mean_squared_error', verbose=2, save_best_only=True) # con questo fa tutte le epoche ma salva il migliore. SOpra può fermarsi prima di fine epoche\n",
    "history=model.fit(X_train_vect,Y_train,\n",
    "\t\t\t\t\tepochs=epoche,\n",
    "\t\t\t\t\tvalidation_data=(X_val_vect,Y_val),\n",
    "\t\t\t\t\tbatch_size=b_size,\n",
    "\t\t\t\t\tcallbacks=[es],\n",
    "\t\t\t\t\tverbose=verb)\n",
    "\n",
    "print('\\n Test accuracy = ', model.evaluate(X_test_vect,Y_test, verbose=0)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e18c37fd15a1974bcbd8e3537474f367f1f4a053e6ef9eedc221dd433503edc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
