{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8c302d-65eb-4a99-b7b4-fac1333986bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# N-gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b0544-e1af-43c0-beaf-a19b047f9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902dd652-441b-439e-90fd-3c9c3ad0ef93",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "\n",
    "The dataset that we will use is the DreamBank dataset (https://huggingface.co/datasets/DReAMy-lib/DreamBank-dreams-en). The dataset is a collection of ~20 k textual reports of dreams, originally scraped from the DreamBank databased by mattbierner. The DreamBank reports are divided into series, which are collections of individuals or research projects/groups that have gathered the dreams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9688d96-218a-4bee-b80a-a87ec677dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"DReAMy-lib/DreamBank-dreams-en\")\n",
    "\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "df[\"dreams\"] = df[\"dreams\"].astype(str)\n",
    "\n",
    "train_df = df.query(\"series != 'natural_scientist'\")\n",
    "train_list = list(train_df[\"dreams\"])\n",
    "print(\"Number of dream reports in the training set:\",len(train_list))\n",
    "\n",
    "test_df = df.query(\"series == 'natural_scientist'\")\n",
    "test_list = list(test_df[\"dreams\"])\n",
    "print(\"Number of dream reports in the test set:\",len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474ec65-c86c-404a-9203-d16bfe06f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_strings = \" \".join(train_list)\n",
    "train_strings = train_strings.lower()\n",
    "train_tokens = word_tokenize(train_strings)\n",
    "print(\"Number of tokens in the training set:\",len(train_tokens))\n",
    "\n",
    "vocab = set(train_tokens)\n",
    "print(\"Vocabulary size:\",len(vocab))\n",
    "\n",
    "test_strings = \" \".join(test_list)\n",
    "test_strings = test_strings.lower()\n",
    "test_tokens = word_tokenize(test_strings)\n",
    "print(\"Number of tokens in the test set:\",len(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f26cb-20c6-4dc5-96b7-4e7740e1a27d",
   "metadata": {},
   "source": [
    "## Import dataset and prepare training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd22bd-1454-46ea-8467-99476ebfac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, StupidBackoff, Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed5c71-ec83-4e0c-8468-860c95381140",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = sent_tokenize(train_strings)\n",
    "print(len(train_sents))\n",
    "train_sents_tokens = [word_tokenize(s) for s in train_sents]\n",
    "\n",
    "test_sents = sent_tokenize(test_strings)\n",
    "print(len(test_sents))\n",
    "test_sents_tokens = [word_tokenize(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05025e-0192-4457-b826-88be9af0b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bcc6c-02ac-452b-b5a0-59b6f093b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(n, train_sents_tokens)\n",
    "\n",
    "#lm_mle = MLE(n) # Maximum Likelihood Estimate\n",
    "#lm_sb = StupidBackoff(order = n) # Stupid Backoff\n",
    "lm_laplace = Laplace(n) # Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd176d-f4a0-4f2c-8d70-5ffc0804ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.fit(train, vocab)\n",
    "print(lm_laplace.vocab)\n",
    "print(len(lm_laplace.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308ed30-2eb0-4987-966a-32fc56dd7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm_laplace.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716ef3a-16e8-4ea7-9956-80fed4fd7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.counts['dream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405f101-c325-48ac-9ccb-0a483103c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.counts[['i']]['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad8699-46b0-484e-aa12-619e0154126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.counts[['i']]['want']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad39c9-4768-4ef2-b6b5-d6be0894f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.score(\"i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a74a0b-4889-424e-b771-56a2788282dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.score(\"davide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23210af9-686c-4007-9882-17afa4e59395",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.perplexity(train_sents_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067bfc8-0d28-4edd-8597-f469c7958cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.perplexity(test_sents_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8d6ad-f125-4ac6-8680-1275af7218c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde7ae5-5d7b-4c55-b1e3-9604b15edae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_laplace.generate(20, text_seed=['so'], random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4da595-8ae2-4e8e-bef4-8e1b653c2dd4",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Compute and compare the perplexity for 2-grams LMs (MLE vs Laplace vs Stupid Backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9d5f8-c8be-45a9-b760-856020303878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
