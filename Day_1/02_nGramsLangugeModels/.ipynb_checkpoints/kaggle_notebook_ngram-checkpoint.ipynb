{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9186,"sourceType":"datasetVersion","datasetId":6261}],"dockerImageVersionId":30042,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This kernel aims to expand on what I've learned from the deeplearning.ai NLP Specialisation, one of the assignments for that course used **n-gram models** to build a auto-completion program. Most of this kernel is ported from that assignment. The dataset used for this kernel is very similar to the one used for the assignment.\n\nThe aim is to convert this into a web application and deploy it to Heroku using streamlit. The Streamlit App can be found [here](https://autocomplete-ngram.herokuapp.com/) Takes a while to load üòÖ. Below is the screenshot of how the Application looks like. [Link to the Github repository](https://github.com/SauravMaheshkar/Auto-Completion-using-N-Gram-Models).\n\n![App Screenshot](https://raw.githubusercontent.com/SauravMaheshkar/Auto-Completion-using-N-Gram-Models/master/assets/app.png)\n\n####  If you liked this project and would like to read the code and see some of my other work, don't forget to ‚≠ê the [repository](https://github.com/SauravMaheshkar/Auto-Completion-using-N-Gram-Models) and follow [me](https://github.com/SauravMaheshkar).","metadata":{}},{"cell_type":"markdown","source":"# Table of Content\n\n* [üìö Theory](#section-one)\n    * [The Bigram Model ‚ë°](#bigram-model)\n    * [Estimation ‚©∞](#estimation)\n* [üìÇ Basic Setup](#basic-setup)\n* [üßΩ Pre-Processing pipeline](#pre-process)\n* [‚úÇÔ∏è Splitting into Train, Valid and Test](#split)\n* [üßπ Cleaning the Data](#clean)\n    * [üìî Creating a Frequency Dictionary](#frequency)\n    * [üîí Creating a Closed Vocabulary](#closed) \n    * [ü§∑üèª Adding UNK Tokens](#unk)\n    * [üßº Final Cleaning Pipeline](#final)\n* [üí™üèª Building The \"Model\"](#build)\n* [üí¨ The Auto-Complete System](#auto-complete)\n* [üòä Inference](#inference)\n* [üßê Miscellaneous](#misc)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# üìö Theory\n\nLet's delve into the theory and try to gain a intuition about n-gram language models.\n\nN-Gram models are Statistical(Probabilistic) Language models that aim to assign probabilities to a given sequence of words. Any N-gram is just a sequence of \"n\" words. For example, \"Saurav\" is a unigram and \"Hi There\" is a bigram. \n\n\nThe task is to find out if we can compute $P(w | h)$ given a word $w$ and some history $h$. One could say that we can compute the probability of a given next word, using all the previous words in the sentence. For example using the last sentence, we could calculate: \n\n$$\\large\nP ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)\n$$\n\n---\n\nOne such approach could be to use **relative frequency counts** to compute this probability, i.e. ,**Out of the times we saw the history $h$, how many times was it followed by the word $w$**\n\nOr \n\n$$\nP ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,) = \\frac{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\, word)}{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)}\n$$\n---\n\nIntuitively it seems infeasible to perform this over an entire corpus; especially it is of a significant a size. This is the motivation behind the N-gram model, instead of using the entire corpus, we approximate this probability using just `n` previous words.\n\nFor instance if $w_{1:n}$ represents the sequence of words $w_1w_2...w_n$, then using the chain rule of probability we can write,  \n\n\n$$\\large\nP(w_{1:n}) = P(w_1)P(w_2 | w_1)P(w_3 | w_{1:2})...P(w_n|w_{1:n-1})\n$$\n\n\n$$\\large\nP(w_{1:n}) = \\prod_{k=1}^{n}P(w_k | w_{1:k-1})\n$$\n\n<a id=\"bigram-model\"></a>\n## The Bigram Model ‚ë°\n\nA Bigram Model corresponds to a model which approximates the probability of a word given all the previous words $P(w_n|w_{1:n‚àí1})$ by using only the conditional probability of the preceding word $P(w_n|w_{n‚àí1})$. Thus we assume that $P(w_n|w_{1:n‚àí1}) ‚âà P(w_n|w_{n‚àí1})$. This approximation is known as the **Markov** approximation. Thus, for the Bigram model, the probability for an entire sequence can be approximated as:\n\n$$\\large\nP(w_{1:n}) ‚âà \\prod_{k=1}^{n}P(w_{k}|w_{k‚àí1}) \n$$\n\n<a id=\"estimation\"></a>\n## Estimation ‚©∞\n\nTo estimate such probabilities we use the **Maximum Likelihood Estimation (MLE)**. An MLE estimate for the parameters of an n-gram model can be obtained by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.\n\nFor a Bigram model, the MLE Estimation can be given by:\n\n$$\\large\nP(w_n | w_{n-1}) \\frac{C(w_{n-1}w_n)}{\\sum_{w} C(w_{n-1}w)}\n$$\n\n---\nFor the general case of MLE n-gram parameter estimation:\n\n$$\\large\nP(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n$$","metadata":{}},{"cell_type":"markdown","source":"<a id=\"basic-setup\"></a>\n# üìÇ Basic Setup","metadata":{}},{"cell_type":"markdown","source":"In this **hidden** code cell, we'll import our packages. As we'll implement n-gram models from scratch we'll just use numpy and Additionally nltk  (just for Tokenization).","metadata":{}},{"cell_type":"code","source":"%%capture\n\n## Importing Packages\nimport math\nimport nltk\nimport random\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n## Basic File Paths\ndata_dir = \"../input/tweets-blogs-news-swiftkey-dataset-4million/final/en_US\"\nfile_path = data_dir + \"/en_US.twitter.txt\"\n\n## nltk settings\nnltk.data.path.append(data_dir)\nnltk.download('punkt')\n\n## Opening the File in read mode (\"r\")\nwith open(file_path, \"r\") as f:\n    data = f.read()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"pre-process\"></a>\n# üßΩ Pre-Processing pipeline","metadata":{}},{"cell_type":"markdown","source":"We create a simple pipeline function which: \n\n* splits the datasets by the `\\n` character \n\n* remove leading and trailing spaces \n\n* drop empty sentences. \n\n* Tokenize sentences using `nltk.word_tokenize`","metadata":{}},{"cell_type":"code","source":"def preprocess_pipeline(data) -> 'list':\n\n    # Split by newline character\n    sentences = data.split('\\n')\n    \n    # Remove leading and trailing spaces\n    sentences = [s.strip() for s in sentences]\n    \n    # Drop Empty Sentences\n    sentences = [s for s in sentences if len(s) > 0]\n    \n    # Empty List to hold Tokenized Sentences\n    tokenized = []\n    \n    # Iterate through sentences\n    for sentence in sentences:\n        \n        # Convert to lowercase\n        sentence = sentence.lower()\n        \n        # Convert to a list of words\n        token = nltk.word_tokenize(sentence)\n        \n        # Append to list\n        tokenized.append(token)\n        \n    return tokenized\n\n\n## Pass our data to this function    \ntokenized_sentences = preprocess_pipeline(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"split\"></a>\n# ‚úÇÔ∏è Splitting into Train, Valid and Test","metadata":{}},{"cell_type":"code","source":"## Obtain Train and Test Split \ntrain, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n\n## Obtain Train and Validation Split \ntrain, val = train_test_split(train, test_size=0.25, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"clean\"></a>\n# üßπ Cleaning the Data","metadata":{}},{"cell_type":"markdown","source":"<a id=\"frequency\"></a>\n## üìî Creating a Frequency Dictionary","metadata":{}},{"cell_type":"markdown","source":"As our dataset is quite big, we'll only use those words that appear `k` times in our dataset. In this function, we'll create a frequency dictionary for our vocabulary. ","metadata":{}},{"cell_type":"code","source":"def count_the_words(sentences) -> 'dict':\n    \n  # Creating a Dictionary of counts\n  word_counts = {}\n\n  # Iterating over sentences\n  for sentence in sentences:\n    \n    # Iterating over Tokens\n    for token in sentence:\n    \n      # Add count for new word\n      if token not in word_counts.keys():\n        word_counts[token] = 1\n        \n      # Increase count by one\n      else:\n        word_counts[token] += 1\n        \n  return word_counts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"closed\"></a>\n## üîí Creating a Closed Vocabulary","metadata":{}},{"cell_type":"markdown","source":"One of the most essential steps in dealing with Textual data is handling Out-of-vocabulary words. This helps the model to handle words which are not present in the training corpus. First step in this process is to create a `closed_vocabulary`. This function creates a closed vocabulary containing only those words according to the `count_threshold` parameter.","metadata":{}},{"cell_type":"code","source":"def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n\n  # Empty list for closed vocabulary\n  closed_vocabulary = []\n\n  # Obtain frequency dictionary using previously defined function\n  words_count = count_the_words(tokenized_sentences)\n    \n  # Iterate over words and counts \n  for word, count in words_count.items():\n    \n    # Append if it's more(or equal) to the threshold \n    if count >= count_threshold :\n      closed_vocabulary.append(word)\n\n  return closed_vocabulary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"unk\"></a>\n## ü§∑üèª Adding UNK Tokens","metadata":{}},{"cell_type":"markdown","source":"In this function we'll add `<unk>` tokens, to those words which are not in the `closed_vocabulary` which we just made.","metadata":{}},{"cell_type":"code","source":"def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n\n  # Convert Vocabulary into a set\n  vocabulary = set(vocabulary)\n\n  # Create empty list for sentences\n  new_tokenized_sentences = []\n  \n  # Iterate over sentences\n  for sentence in tokenized_sentences:\n\n    # Iterate over sentence and add <unk> \n    # if the token is absent from the vocabulary\n    new_sentence = []\n    for token in sentence:\n      if token in vocabulary:\n        new_sentence.append(token)\n      else:\n        new_sentence.append(unknown_token)\n    \n    # Append sentece to the new list\n    new_tokenized_sentences.append(new_sentence)\n\n  return new_tokenized_sentences","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"final\"></a>\n## üßº Final Cleaning Pipeline","metadata":{}},{"cell_type":"code","source":"def cleansing(train_data, test_data, count_threshold):\n    \n  # Get closed Vocabulary\n  vocabulary = handling_oov(train_data, count_threshold)\n    \n  # Updated Training Dataset\n  new_train_data = unk_tokenize(train_data, vocabulary)\n    \n  # Updated Test Dataset\n  new_test_data = unk_tokenize(test_data, vocabulary)\n\n  return new_train_data, new_test_data, vocabulary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_freq = 6\nfinal_train, final_test, vocabulary = cleansing(train, test, min_freq)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"build\"></a>\n# üí™üèª Building The \"Model\"","metadata":{}},{"cell_type":"markdown","source":"This is a helper function, which will come in handy during inference. This function returns a mapping from n-grams to their frequency in the dataset. ","metadata":{}},{"cell_type":"code","source":"def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n\n  # Empty dict for n-grams\n  n_grams = {}\n \n  # Iterate over all sentences in the dataset\n  for sentence in data:\n        \n    # Append n start tokens and a single end token to the sentence\n    sentence = [start_token]*n + sentence + [end_token]\n    \n    # Convert the sentence into a tuple\n    sentence = tuple(sentence)\n\n    # Temp var to store length from start of n-gram to end\n    m = len(sentence) if n==1 else len(sentence)-1\n    \n    # Iterate over this length\n    for i in range(m):\n        \n      # Get the n-gram\n      n_gram = sentence[i:i+n]\n    \n      # Add the count of n-gram as value to our dictionary\n      # IF n-gram is already present\n      if n_gram in n_grams.keys():\n        n_grams[n_gram] += 1\n      # Add n-gram count\n      else:\n        n_grams[n_gram] = 1\n        \n  return n_grams","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function calculates the priority for the next word given the prior n-gram. This function also implements k-smoothing which helps account for unseen n-grams. Using the previously defined formula:\n\n\n$$\\large\nP(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n$$\n\n---\n\n### K-smoothing\n\nBut what if we come across a n-gram that wasn't in the training set. Then our denominator would would become zero and our definition of probability will become invalid. Thus, we use k-smoothing, which adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary. This ensures any n-gram with zero count has the same probability of $\\frac{1}{|V|}$. Thus, our original estimation get's modified to:\n\n$$\\large\nP(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n) + k}{C(w_{n‚àíN+1:n‚àí1} + k |V|)}\n$$","metadata":{}},{"cell_type":"code","source":"def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n\n  # Convert the previous_n_gram into a tuple \n  previous_n_gram = tuple(previous_n_gram)\n    \n  # Calculating the count, if exists from our freq dictionary otherwise zero\n  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n  \n  # The Denominator\n  denom = previous_n_gram_count + k * vocabulary_size\n\n  # previous n-gram plus the current word as a tuple\n  nplus1_gram = previous_n_gram + (word,)\n\n  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n\n  # Numerator\n  num = nplus1_gram_count + k\n\n  # Final Fraction\n  prob = num / denom\n  return prob","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we loop over all the words in the vocabulary and then compute their probabilites using our `prob_for_single_word()` fn.","metadata":{}},{"cell_type":"code","source":"def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n\n  # Convert to Tuple\n  previous_n_gram = tuple(previous_n_gram)\n\n  # Add end and unknown tokens to the vocabulary\n  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n\n  # Calculate the size of the vocabulary\n  vocabulary_size = len(vocabulary)\n\n  # Empty dict for probabilites\n  probabilities = {}\n\n  # Iterate over words \n  for word in vocabulary:\n    \n    # Calculate probability\n    probability = prob_for_single_word(word, previous_n_gram, \n                                           n_gram_counts, nplus1_gram_counts, \n                                           vocabulary_size, k=k)\n    # Create mapping: word -> probability\n    probabilities[word] = probability\n\n  return probabilities","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"auto-complete\"></a>\n# üí¨ The Auto-Complete System","metadata":{}},{"cell_type":"markdown","source":"Finally, we build our `auto_complete` fn. We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability. ","metadata":{}},{"cell_type":"code","source":"def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n\n    \n    # length of previous words\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # most recent 'n' words\n    previous_n_gram = previous_tokens[-n:]\n    \n    # Calculate probabilty for all words\n    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n\n    # Intialize the suggestion and max probability\n    suggestion = None\n    max_prob = 0\n\n    # Iterate over all words and probabilites, returning the max.\n    # We also add a check if the start_with parameter is provided\n    for word, prob in probabilities.items():\n        \n        if start_with != None: \n            \n            if not word.startswith(start_with):\n                continue \n\n        if prob > max_prob: \n\n            suggestion = word\n            max_prob = prob\n\n    return suggestion, max_prob","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also loop over all the various n-gram models to get multiple suggestions. This function just extends from the previously defined function by **taking multiple n-gram counts** instead of one. This allows us to take unigram, bigram, .. counts into account as well.","metadata":{}},{"cell_type":"code","source":"def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n\n    # See how many models we have\n    count = len(n_gram_counts_list)\n    \n    # Empty list for suggestions\n    suggestions = []\n    \n    # IMP: Earlier \"-1\"\n    \n    # Loop over counts\n    for i in range(count-1):\n        \n        # get n and nplus1 counts\n        n_gram_counts = n_gram_counts_list[i]\n        nplus1_gram_counts = n_gram_counts_list[i+1]\n        \n        # get suggestions \n        suggestion = auto_complete(previous_tokens, n_gram_counts,\n                                    nplus1_gram_counts, vocabulary,\n                                    k=k, start_with=start_with)\n        # Append to list\n        suggestions.append(suggestion)\n        \n    return suggestions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"inference\"></a>\n# üòä Inference","metadata":{}},{"cell_type":"markdown","source":"Here, we create a list of n-gram counts for a arbitrary range `(1,6)`","metadata":{}},{"cell_type":"code","source":"n_gram_counts_list = []\nfor n in range(1, 6):\n    n_model_counts = count_n_grams(final_train, n)\n    n_gram_counts_list.append(n_model_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's give it a sample input of \"i was about\" in a tokenized manner and get multiple suggestions using the above calculated n-gram counts with smoothing-factor, `k` = 1.0 ","metadata":{}},{"cell_type":"code","source":"previous_tokens = [\"i\", \"was\", \"about\"]\nsuggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\ndisplay(suggestion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"misc\"></a>\n# üßê Miscellaneous ","metadata":{}},{"cell_type":"markdown","source":"Let's see how many n-grams we have in our corpus.","metadata":{}},{"cell_type":"code","source":"print(\"unigram count:\" , len(n_gram_counts_list[0]))\nprint(\"bigram count:\", len(n_gram_counts_list[1]))\nprint(\"trigram count:\", len(n_gram_counts_list[2]))\nprint(\"quadgram count:\", len(n_gram_counts_list[3]))\nprint(\"quintgram count:\", len(n_gram_counts_list[4]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this section, we just export this list to a `.txt` file so that we can use this for inference rather than \"training\" each time.","metadata":{}},{"cell_type":"code","source":"# Storing to file\nwith open(\"en_counts.txt\", 'wb') as f:\n    pickle.dump(n_gram_counts_list, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Storing to file\nwith open(\"vocab.txt\", 'wb') as f:\n    pickle.dump(vocabulary, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}