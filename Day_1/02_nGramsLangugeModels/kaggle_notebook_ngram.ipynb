{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-one\"></a>\n",
    "# üìö Theory\n",
    "\n",
    "Let's delve into the theory and try to gain a intuition about n-gram language models.\n",
    "\n",
    "N-Gram models are Statistical(Probabilistic) Language models that aim to assign probabilities to a given sequence of words. Any N-gram is just a sequence of \"n\" words. For example, \"Saurav\" is a unigram and \"Hi There\" is a bigram. \n",
    "\n",
    "\n",
    "The task is to find out if we can compute $P(w | h)$ given a word $w$ and some history $h$. One could say that we can compute the probability of a given next word, using all the previous words in the sentence. For example using the last sentence, we could calculate: \n",
    "\n",
    "$$\\large\n",
    "P ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "One such approach could be to use **relative frequency counts** to compute this probability, i.e. ,**Out of the times we saw the history $h$, how many times was it followed by the word $w$**\n",
    "\n",
    "Or \n",
    "\n",
    "$$\n",
    "P ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,) = \\frac{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\, word)}{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)}\n",
    "$$\n",
    "---\n",
    "\n",
    "Intuitively it seems infeasible to perform this over an entire corpus; especially it is of a significant a size. This is the motivation behind the N-gram model, instead of using the entire corpus, we approximate this probability using just `n` previous words.\n",
    "\n",
    "For instance if $w_{1:n}$ represents the sequence of words $w_1w_2...w_n$, then using the chain rule of probability we can write,  \n",
    "\n",
    "\n",
    "$$\\large\n",
    "P(w_{1:n}) = P(w_1)P(w_2 | w_1)P(w_3 | w_{1:2})...P(w_n|w_{1:n-1})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\large\n",
    "P(w_{1:n}) = \\prod_{k=1}^{n}P(w_k | w_{1:k-1})\n",
    "$$\n",
    "\n",
    "<a id=\"bigram-model\"></a>\n",
    "## The Bigram Model ‚ë°\n",
    "\n",
    "A Bigram Model corresponds to a model which approximates the probability of a word given all the previous words $P(w_n|w_{1:n‚àí1})$ by using only the conditional probability of the preceding word $P(w_n|w_{n‚àí1})$. Thus we assume that $P(w_n|w_{1:n‚àí1}) ‚âà P(w_n|w_{n‚àí1})$. This approximation is known as the **Markov** approximation. Thus, for the Bigram model, the probability for an entire sequence can be approximated as:\n",
    "\n",
    "$$\\large\n",
    "P(w_{1:n}) ‚âà \\prod_{k=1}^{n}P(w_{k}|w_{k‚àí1}) \n",
    "$$\n",
    "\n",
    "<a id=\"estimation\"></a>\n",
    "## Estimation ‚©∞\n",
    "\n",
    "To estimate such probabilities we use the **Maximum Likelihood Estimation (MLE)**. An MLE estimate for the parameters of an n-gram model can be obtained by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.\n",
    "\n",
    "For a Bigram model, the MLE Estimation can be given by:\n",
    "\n",
    "$$\\large\n",
    "P(w_n | w_{n-1}) \\frac{C(w_{n-1}w_n)}{\\sum_{w} C(w_{n-1}w)}\n",
    "$$\n",
    "\n",
    "---\n",
    "For the general case of MLE n-gram parameter estimation:\n",
    "\n",
    "$$\\large\n",
    "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davideposillipo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Basic File Paths\n",
    "data_dir = \"../../data/final/en_US\"\n",
    "file_path = data_dir + \"/en_US.twitter.txt\"\n",
    "\n",
    "## nltk settings\n",
    "#nltk.data.path.append(data_dir)\n",
    "nltk.download('punkt')\n",
    "\n",
    "## Opening the File in read mode (\"r\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple pipeline function which: \n",
    "\n",
    "* splits the datasets by the `\\n` character \n",
    "\n",
    "* remove leading and trailing spaces \n",
    "\n",
    "* drop empty sentences. \n",
    "\n",
    "* Tokenize sentences using `nltk.word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(data) -> 'list':\n",
    "\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    tokenized = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        token = nltk.word_tokenize(sentence)\n",
    "        tokenized.append(token)\n",
    "        \n",
    "    return tokenized\n",
    "\n",
    "tokenized_sentences = preprocess_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360148"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset is quite big, we'll only use those words that appear `k` times in our dataset. In this function, we'll create a frequency dictionary for our vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_words(sentences) -> 'dict':\n",
    "    \n",
    "  # Creating a Dictionary of counts\n",
    "  word_counts = {}\n",
    "\n",
    "  # Iterating over sentences\n",
    "  for sentence in sentences:\n",
    "    \n",
    "    # Iterating over Tokens\n",
    "    for token in sentence:\n",
    "    \n",
    "      # Add count for new word\n",
    "      if token not in word_counts.keys():\n",
    "        word_counts[token] = 1\n",
    "        \n",
    "      # Increase count by one\n",
    "      else:\n",
    "        word_counts[token] += 1\n",
    "        \n",
    "  return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"closed\"></a>\n",
    "## üîí Creating a Closed Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most essential steps in dealing with Textual data is handling Out-of-vocabulary words. This helps the model to handle words which are not present in the training corpus. First step in this process is to create a `closed_vocabulary`. This function creates a closed vocabulary containing only those words according to the `count_threshold` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n",
    "\n",
    "  # Empty list for closed vocabulary\n",
    "  closed_vocabulary = []\n",
    "\n",
    "  # Obtain frequency dictionary using previously defined function\n",
    "  words_count = count_the_words(tokenized_sentences)\n",
    "    \n",
    "  # Iterate over words and counts \n",
    "  for word, count in words_count.items():\n",
    "    \n",
    "    # Append if it's more(or equal) to the threshold \n",
    "    if count >= count_threshold :\n",
    "      closed_vocabulary.append(word)\n",
    "\n",
    "  return closed_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unk\"></a>\n",
    "## ü§∑üèª Adding UNK Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function we'll add `<unk>` tokens, to those words which are not in the `closed_vocabulary` which we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
    "\n",
    "  # Convert Vocabulary into a set\n",
    "  vocabulary = set(vocabulary)\n",
    "\n",
    "  # Create empty list for sentences\n",
    "  new_tokenized_sentences = []\n",
    "  \n",
    "  # Iterate over sentences\n",
    "  for sentence in tokenized_sentences:\n",
    "\n",
    "    # Iterate over sentence and add <unk> \n",
    "    # if the token is absent from the vocabulary\n",
    "    new_sentence = []\n",
    "    for token in sentence:\n",
    "      if token in vocabulary:\n",
    "        new_sentence.append(token)\n",
    "      else:\n",
    "        new_sentence.append(unknown_token)\n",
    "    \n",
    "    # Append sentece to the new list\n",
    "    new_tokenized_sentences.append(new_sentence)\n",
    "\n",
    "  return new_tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final\"></a>\n",
    "## üßº Final Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(train_data, test_data, count_threshold):\n",
    "    \n",
    "  # Get closed Vocabulary\n",
    "  vocabulary = handling_oov(train_data, count_threshold)\n",
    "    \n",
    "  # Updated Training Dataset\n",
    "  new_train_data = unk_tokenize(train_data, vocabulary)\n",
    "    \n",
    "  # Updated Test Dataset\n",
    "  new_test_data = unk_tokenize(test_data, vocabulary)\n",
    "\n",
    "  return new_train_data, new_test_data, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "# üí™üèª Building The \"Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function, which will come in handy during inference. This function returns a mapping from n-grams to their frequency in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
    "\n",
    "  # Empty dict for n-grams\n",
    "  n_grams = {}\n",
    " \n",
    "  # Iterate over all sentences in the dataset\n",
    "  for sentence in data:\n",
    "        \n",
    "    # Append n start tokens and a single end token to the sentence\n",
    "    sentence = [start_token]*n + sentence + [end_token]\n",
    "    \n",
    "    # Convert the sentence into a tuple\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    # Temp var to store length from start of n-gram to end\n",
    "    m = len(sentence) if n==1 else len(sentence)-1\n",
    "    \n",
    "    # Iterate over this length\n",
    "    for i in range(m):\n",
    "        \n",
    "      # Get the n-gram\n",
    "      n_gram = sentence[i:i+n]\n",
    "    \n",
    "      # Add the count of n-gram as value to our dictionary\n",
    "      # IF n-gram is already present\n",
    "      if n_gram in n_grams.keys():\n",
    "        n_grams[n_gram] += 1\n",
    "      # Add n-gram count\n",
    "      else:\n",
    "        n_grams[n_gram] = 1\n",
    "        \n",
    "  return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the priority for the next word given the prior n-gram. This function also implements k-smoothing which helps account for unseen n-grams. Using the previously defined formula:\n",
    "\n",
    "\n",
    "$$\\large\n",
    "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### K-smoothing\n",
    "\n",
    "But what if we come across a n-gram that wasn't in the training set. Then our denominator would would become zero and our definition of probability will become invalid. Thus, we use k-smoothing, which adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary. This ensures any n-gram with zero count has the same probability of $\\frac{1}{|V|}$. Thus, our original estimation get's modified to:\n",
    "\n",
    "$$\\large\n",
    "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n) + k}{C(w_{n‚àíN+1:n‚àí1} + k |V|)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
    "\n",
    "  # Convert the previous_n_gram into a tuple \n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
    "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "  \n",
    "  # The Denominator\n",
    "  denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "  # previous n-gram plus the current word as a tuple\n",
    "  nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
    "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "  # Numerator\n",
    "  num = nplus1_gram_count + k\n",
    "\n",
    "  # Final Fraction\n",
    "  prob = num / denom\n",
    "  return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we loop over all the words in the vocabulary and then compute their probabilites using our `prob_for_single_word()` fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
    "\n",
    "  # Convert to Tuple\n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "  # Add end and unknown tokens to the vocabulary\n",
    "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "  # Calculate the size of the vocabulary\n",
    "  vocabulary_size = len(vocabulary)\n",
    "\n",
    "  # Empty dict for probabilites\n",
    "  probabilities = {}\n",
    "\n",
    "  # Iterate over words \n",
    "  for word in vocabulary:\n",
    "    \n",
    "    # Calculate probability\n",
    "    probability = prob_for_single_word(word, previous_n_gram, \n",
    "                                           n_gram_counts, nplus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "    # Create mapping: word -> probability\n",
    "    probabilities[word] = probability\n",
    "\n",
    "  return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"auto-complete\"></a>\n",
    "# üí¨ The Auto-Complete System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build our `auto_complete` fn. We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    # Calculate probabilty for all words\n",
    "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
    "\n",
    "    # Intialize the suggestion and max probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    # Iterate over all words and probabilites, returning the max.\n",
    "    # We also add a check if the start_with parameter is provided\n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        if start_with != None: \n",
    "            \n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "\n",
    "        if prob > max_prob: \n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also loop over all the various n-gram models to get multiple suggestions. This function just extends from the previously defined function by **taking multiple n-gram counts** instead of one. This allows us to take unigram, bigram, .. counts into account as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    # See how many models we have\n",
    "    count = len(n_gram_counts_list)\n",
    "    \n",
    "    # Empty list for suggestions\n",
    "    suggestions = []\n",
    "    \n",
    "    # IMP: Earlier \"-1\"\n",
    "    \n",
    "    # Loop over counts\n",
    "    for i in range(count-1):\n",
    "        \n",
    "        # get n and nplus1 counts\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        # get suggestions \n",
    "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
    "                                    nplus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        # Append to list\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>\n",
    "# üòä Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a list of n-gram counts for a arbitrary range `(1,6)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(final_train, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a sample input of \"i was about\" in a tokenized manner and get multiple suggestions using the above calculated n-gram counts with smoothing-factor, `k` = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"i\", \"was\", \"about\"]\n",
    "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "display(suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"misc\"></a>\n",
    "# üßê Miscellaneous "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many n-grams we have in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unigram count:\" , len(n_gram_counts_list[0]))\n",
    "print(\"bigram count:\", len(n_gram_counts_list[1]))\n",
    "print(\"trigram count:\", len(n_gram_counts_list[2]))\n",
    "print(\"quadgram count:\", len(n_gram_counts_list[3]))\n",
    "print(\"quintgram count:\", len(n_gram_counts_list[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we just export this list to a `.txt` file so that we can use this for inference rather than \"training\" each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing to file\n",
    "with open(\"en_counts.txt\", 'wb') as f:\n",
    "    pickle.dump(n_gram_counts_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing to file\n",
    "with open(\"vocab.txt\", 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6261,
     "sourceId": 9186,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30042,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
