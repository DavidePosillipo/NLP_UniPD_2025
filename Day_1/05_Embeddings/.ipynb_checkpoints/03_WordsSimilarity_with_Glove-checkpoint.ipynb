{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1c4cea",
   "metadata": {},
   "source": [
    "### Words similarity by GloVe\n",
    "\n",
    "In order to capture in a quantitative way the nuance necessary to distinguish man from woman, it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the <b>vector difference between the two word vectors</b>.  \n",
    "  \n",
    "\n",
    "GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c60352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541baf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "glove_path = \"../../Data/glove/glove.6B.100d.txt\"  # Path to the GloVe file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ec2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14472f54",
   "metadata": {},
   "source": [
    "### Words similarity\n",
    "\n",
    "The semantic meaning of two words is compared with the cosine similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return np.dot(embedding1, embedding2)/(np.linalg.norm(embedding1)*np.linalg.norm(embedding2))\n",
    "\n",
    "def word_similarity(word1, word2, embeddings_index):\n",
    "    word1 = word1.lower()\n",
    "    word2 = word2.lower()\n",
    "\n",
    "    if word1 not in embeddings_index or word2 not in embeddings_index:\n",
    "        return None\n",
    "\n",
    "    embedding1 = embeddings_index[word1]\n",
    "    embedding2 = embeddings_index[word2]\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea575b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a few words we want to analyze with GloVe\n",
    "\n",
    "word1 = 'man'\n",
    "word2 = 'woman'\n",
    "\n",
    "word3 = 'king'\n",
    "word4 = 'queen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be28519",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1 = word_similarity(word1, word2, embeddings_index)\n",
    "similarity_2 = word_similarity(word3, word4, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if similarity_1 is not None:\n",
    "    print(f'Similarity between \"{word1}\"-\"{word2}\": ', end='')\n",
    "    print(\"{0:0.3f}. \".format(similarity_1), end='')\n",
    "    print(\"\\u03F4 = {0:0.1f}° \".format(np.arccos(similarity_1)*180/np.pi))\n",
    "    print(f'Similarity between \"{word3}\"-\"{word4}\": ', end='')\n",
    "    print(\"{0:0.3f}. \".format(similarity_2), end='')\n",
    "    print(\"\\u03F4 = {0:0.1f}° \".format(np.arccos(similarity_2)*180/np.pi))\n",
    "else:\n",
    "    print('One or both words are not present in the GloVe embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbdc23",
   "metadata": {},
   "source": [
    "## Words analogy task\n",
    "\n",
    "How do we know whether the different semantic meaning between two words is similar to that between another pair of words?\n",
    "\n",
    "This is a relevant information, exploited by text generation engines.\n",
    "\n",
    "You need to find the word that completes the following proportion:  \n",
    "_man_ __is to__ _king_ __as__ _woman_ __is to__ [...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_similarity(pair1, pair2, embeddings_index):\n",
    "    word1 = pair1[0].lower()\n",
    "    word2 = pair1[1].lower()\n",
    "    word3 = pair2[0].lower()\n",
    "    word4 = pair2[1].lower()\n",
    "\n",
    "    if word1 not in embeddings_index or word2 not in embeddings_index:\n",
    "        return None\n",
    "    if word3 not in embeddings_index or word4 not in embeddings_index:\n",
    "        return None\n",
    "\n",
    "    embedding1 = embeddings_index[word1]\n",
    "    embedding2 = embeddings_index[word2]\n",
    "    embedding3 = embeddings_index[word3]\n",
    "    embedding4 = embeddings_index[word4]\n",
    "    \n",
    "    # the following vectors play the difference, with regard to word_similarity function\n",
    "    vec1 = embedding1 - embedding2\n",
    "    vec2 = embedding3 - embedding4\n",
    "    pair_similarity = cosine_similarity(vec1, vec2)\n",
    "\n",
    "    return pair_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a77ca9-4d7b-4671-81bb-13021f15ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a few words we want to analyze with GloVe\n",
    "\n",
    "word1 = 'man'\n",
    "word2 = 'king'\n",
    "\n",
    "word3 = 'woman'\n",
    "word4 = 'queen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_1 = [word1, word2]\n",
    "pair_2 = [word3, word4]\n",
    "pair_sim = pair_similarity(pair_1, pair_2, embeddings_index)\n",
    "print(f'Similarity between \"{word1}\" - \"{word2}\" and \"{word3}\" - \"{word4}\": ', end='')\n",
    "print(\"{0:0.3f}. \".format(pair_sim), end='')\n",
    "print(\"\\u03F4 = {0:0.1f}° \".format(np.arccos(pair_sim)*180/np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c4b26-c98e-45cf-adab-3a2f4a63ec98",
   "metadata": {},
   "source": [
    "### Find closest words for any given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings_cosine(embedding,embeddings_index): \n",
    "    return sorted(embeddings_index.keys(), key=lambda word: cosine_similarity(embeddings_index[word], embedding), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vec = embeddings_index[word1]\n",
    "print(find_closest_embeddings_cosine(input_vec,embeddings_index)[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faca13b1-f1d4-418a-9be1-388f77620794",
   "metadata": {},
   "source": [
    "### Complete analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1860b-97f3-4059-ab6a-cc308b606f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(word1, word2, word3, embeddings_index): \n",
    "    '''\n",
    "    Find x such that (word1 - word2) most similar to (word3 - x)\n",
    "    '''\n",
    "    word1 = word1.lower()\n",
    "    word2 = word2.lower()\n",
    "    word3 = word3.lower()\n",
    "\n",
    "    if word1 not in embeddings_index or word2 not in embeddings_index or word3 not in embeddings_index:\n",
    "        return None\n",
    "\n",
    "    embedding1 = embeddings_index[word1]\n",
    "    embedding2 = embeddings_index[word2]\n",
    "    embedding3 = embeddings_index[word3]\n",
    "\n",
    "    words = embeddings_index.keys()# all words in dictionary\n",
    "    max_cosine_sim = -1 \n",
    "    words_cl = []\n",
    "    cosine_sim=[]\n",
    "    results = pd.DataFrame(columns=['words','cosine_sim'])\n",
    "    for w in words:\n",
    "        # to avoid best_word being one the input words, skip the input word_c\n",
    "        # skip word_c from query\n",
    "        if w in (word1,word2,word3):\n",
    "            continue\n",
    "        words_cl.append(w)\n",
    "        cosine_sim.append(cosine_similarity(embedding1 - embedding2, embedding3 - embeddings_index[w]))\n",
    "        #cosine_sim.append(cosine_similarity(embedding2 - embedding1, embeddings_index[w] - embedding3)) # it is equivalent\n",
    "    results = pd.DataFrame({'words':words_cl,'cosine_sim':cosine_sim})\n",
    "    results = results.sort_values(by='cosine_sim', ascending=False, ignore_index=True)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383fd95-ecdc-4a06-8cfa-e64286c96970",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = find_analogy('man', 'king', 'woman', embeddings_index)\n",
    "res.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32736bc9",
   "metadata": {},
   "source": [
    "### Gender bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the difference between words female and male represents the abstract concept of gender\n",
    "gender = embeddings_index['woman']-embeddings_index['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the vector representing gender is negatively correlated with male names, and positively with female names: expected\n",
    "names = ['daniel','james','william', 'jhon', 'emma', 'alice' ,'sophia','charlotte']\n",
    "for n in names:\n",
    "    print(n,' = ',cosine_similarity(embeddings_index[n],gender))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4ae5a-5259-49c4-9706-7c68a7076004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the vector representing gender is correlated with some professions\n",
    "professions = ['engineer','lawyer', 'warrior','doctor', 'nurse', 'receptionist', 'teacher' ,'singer']\n",
    "for n in professions:\n",
    "    print(n,' = ',cosine_similarity(embeddings_index[n],gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e024c79-5f4a-4cec-8070-3c48f4db113d",
   "metadata": {},
   "source": [
    "### Appendix a: alternative similarity computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f323ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "# define (euclidean) distance function \n",
    "def find_closest_embeddings(embedding,embeddings_index): \n",
    "    return sorted(embeddings_index.keys(), key=lambda word: spatial.distance.euclidean(embeddings_index[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff047a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "triade = ['man','king','woman'] # man:king = woman:?\n",
    "#triade = ['king','man','queen']\n",
    "print(find_closest_embeddings(\n",
    "    embeddings_index[triade[1]] - embeddings_index[triade[0]] + embeddings_index[triade[2]],embeddings_index)[0:6]) #top results are similar but not equal to previous results. We are minimizing two different fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5175cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "triade = ['she','doctor','he'] \n",
    "print(find_closest_embeddings(\n",
    "    embeddings_index[triade[1]] - embeddings_index[triade[0]] + embeddings_index[triade[2]],embeddings_index)[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "triade = ['he','doctor','she'] \n",
    "print(find_closest_embeddings(\n",
    "    embeddings_index[triade[1]] - embeddings_index[triade[0]] + embeddings_index[triade[2]],embeddings_index)[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80621c77-f2a1-41f8-9bfe-5218462fa029",
   "metadata": {},
   "source": [
    "### Appendix b: words distance\n",
    "Euclidean distance can be an alternative to cosine similarity (but is always positive!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_raw_distance(word1, word2, embeddings_index):\n",
    "    word1 = word1.lower()\n",
    "    word2 = word2.lower()\n",
    "\n",
    "    if word1 not in embeddings_index or word2 not in embeddings_index:\n",
    "        return None\n",
    "\n",
    "    embedding1 = embeddings_index[word1]\n",
    "    embedding2 = embeddings_index[word2]\n",
    "    distance = np.linalg.norm(embedding1 - embedding2)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'cat'\n",
    "other = ['dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "for w in other:\n",
    "    dist = word_raw_distance(word, w, embeddings_index) # euclidean distance\n",
    "    print(w, float(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vec = embeddings_index[word]\n",
    "find_closest_embeddings(input_vec,embeddings_index)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd98db-8f0a-42cd-9750-bf90dfd67907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
